{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBfyB-7Hytwy"
      },
      "source": [
        "![PZ-banner](https://palimpzest-workloads.s3.us-east-1.amazonaws.com/palimpzest-cropped.png)\n",
        "\n",
        "# Palimpzest Quickstart\n",
        "This notebook contains a sample program to guide you through the features of the Palimpzest (PZ) library. PZ provides a high-level, declarative interface for composing and executing pipelines of semantic operators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-TkUeCFx1et"
      },
      "source": [
        "## Load Private Key(s)\n",
        "1. Click on the \"key\" icon on the left-hand-side of the Colab notebook.\n",
        "2. In the sidebar that opens, click `+ Add new secret`\n",
        "  - **Note:** your secrets are not visible to anyone other than Google and your version of the notebook.\n",
        "3. Enter one or more of the following keys as secrets:\n",
        "  - `OPENAI_API_KEY`\n",
        "  - `TOGETHER_API_KEY`\n",
        "    - You can create a `together.ai` API key [here](https://api.together.ai/) for this demo (it comes with $1 of free API requests)\n",
        "4. Make sure you have toggled `Notebook access` ON\n",
        "5. Execute the cell below to store these keys in notebook environment variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmkh1n8efxA"
      },
      "source": [
        "#### Note: for the changes to take effect, you may need to restart the session (`Runtime > Restart Session`) if you've already connected the notebook to a runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DgUrHNtZu0z"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# set environment variables\n",
        "def set_api_key_from_secret(key_name):\n",
        "  try:\n",
        "    os.environ[key_name] = userdata.get(key_name)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "set_api_key_from_secret('OPENAI_API_KEY')\n",
        "set_api_key_from_secret('TOGETHER_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNFA4gTzxvE2"
      },
      "source": [
        "## Install Palimpzest\n",
        "First, let's install the Palimpzest package. This may take a few minutes. **PIP dependency error messages are expected and can be ignored.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4AxQGqXIyXsP"
      },
      "outputs": [],
      "source": [
        "!pip install palimpzest==0.7.6\n",
        "!pip install --upgrade pyarrow\n",
        "!pip install chromadb==0.6.3\n",
        "import palimpzest as pz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSAC96Rb-Ggy"
      },
      "source": [
        "## Download Test Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUwsu8XOzgJd"
      },
      "source": [
        "Next, we'll download the dataset we need for this demo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IXv-pxMhx0i1"
      },
      "outputs": [],
      "source": [
        "# download tar files with testdata\n",
        "!wget -nc https://people.csail.mit.edu/gerarvit/PalimpzestData/enron-tiny.tar.gz\n",
        "!wget -nc wget -nc https://people.csail.mit.edu/gerarvit/PalimpzestData/real-estate-eval-5.tar.gz\n",
        "!wget -nc https://palimpzest-workloads.s3.us-east-1.amazonaws.com/chroma-biodex.tar.gz\n",
        "\n",
        "# open tar files\n",
        "!tar -xzf enron-tiny.tar.gz\n",
        "!tar -xzf real-estate-eval-5.tar.gz\n",
        "!tar -xzf chroma-biodex.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw5mmyAY_EaS"
      },
      "source": [
        "# First PZ Program: Filtering Enron Emails\n",
        "For this demo, we will work with a small subset of the Enron Email Dataset to identify emails matching some search criteria.\n",
        "\n",
        "We are going to use Palimpzest to perform the following tasks:\n",
        "1. Load the text files that contain the emails. (Each `.txt` file contains a single email).\n",
        "2. Compute the sender, subject, and date of each email.\n",
        "3. Filter the emails for ones that mention a vacation plan and were sent in the month of July.\n",
        "\n",
        "We can compose these tasks into a PZ program as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lE8xx1s7xoQQ"
      },
      "outputs": [],
      "source": [
        "# define the fields we wish to compute\n",
        "email_cols = [\n",
        "    {\"name\": \"sender\", \"type\": str, \"desc\": \"The email address of the sender\"},\n",
        "    {\"name\": \"subject\", \"type\": str, \"desc\": \"The subject of the email\"},\n",
        "    {\"name\": \"date\", \"type\": str, \"desc\": \"The date the email was sent\"},\n",
        "]\n",
        "\n",
        "# lazily construct the computation to get emails about holidays sent in July\n",
        "dataset = pz.Dataset(\"enron-tiny/\")\n",
        "dataset = dataset.sem_add_columns(email_cols)\n",
        "dataset = dataset.sem_filter(\"The email was sent in July\")\n",
        "dataset = dataset.sem_filter(\"The email is about holidays\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRYDgD3RsMCo"
      },
      "source": [
        "First, we define the set of columns we want to compute in `email_cols`.\n",
        "\n",
        "Next, we create a dataset by simply constructing `pz.Dataset()` with to the path to our files.\n",
        "\n",
        "We then instruct PZ to compute the email columns with a call to `sem_add_columns()`.\n",
        "\n",
        "Finally, we apply our two natural language filters with `sem_filter()`.\n",
        "\n",
        "**Note:** due to PZ's lazy execution, the code above will not execute the PZ program. It simply defines the semantic computation graph.\n",
        "\n",
        "In the next cell, we execute the PZ program with the goal of optimizing for quality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSbS7uC7tUyS"
      },
      "outputs": [],
      "source": [
        "# execute the computation w/the MaxQuality policy\n",
        "config = pz.QueryProcessorConfig(policy=pz.MaxQuality(), execution_strategy=\"parallel\", progress=True)\n",
        "output = dataset.run(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hAjI6JrtbIU"
      },
      "source": [
        "Once our pipeline completes, we can convert the output to a Pandas DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyTrabGGtaZq"
      },
      "outputs": [],
      "source": [
        "# display output (if using Jupyter, otherwise use print(output_df))\n",
        "output_df = output.to_df(cols=[\"date\", \"sender\", \"subject\"])\n",
        "display(output_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55DHU5XNAYBR"
      },
      "source": [
        "Furthermore, Palimpzest provides a detailed report of the execution, with statistics about the runtime and cost of each operation, as well as the final plan that PZ executed.\n",
        "\n",
        "These statistics are stored in `output.execution_stats`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ottmnW4OAhXv"
      },
      "outputs": [],
      "source": [
        "print(f\"Optimization Time: {output.execution_stats.optimization_time:.2f}s\")\n",
        "print(f\"Optimization Cost: ${output.execution_stats.optimization_cost:.3f}\")\n",
        "print(\"---\")\n",
        "print(f\"Plan Execution Time: {output.execution_stats.plan_execution_time:.2f}s\")\n",
        "print(f\"Plan Execution Cost: ${output.execution_stats.plan_execution_cost:.3f}\")\n",
        "\n",
        "print(\"Final plan executed:\")\n",
        "print(\"---\")\n",
        "final_plan_id = list(output.execution_stats.plan_strs.keys())[-1]\n",
        "print(output.execution_stats.plan_strs[final_plan_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm-qRxMyO0i"
      },
      "source": [
        "# Second PZ Program: Multi-Modal Data Processing\n",
        "\n",
        "For our next demo, we will work with a small dataset of five real estate listings to search for properties of interest.\n",
        "\n",
        "We are going to use Palimpzest to execute the following pipeline.\n",
        "1. Load the images and text description for each listing\n",
        "2. Compute the price and address of each listing from the text description\n",
        "3. Filter for homes within our price range\n",
        "4. Filter for homes that look modern and attractive\n",
        "\n",
        "Let's take a moment to visualize the homes in our dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUHkZDwC6EdC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "# Boilerplate code to build our visualization\n",
        "fst_imgs, snd_imgs, thrd_imgs, texts = [], [], [], []\n",
        "for idx in range(1, 6):\n",
        "    listing = f\"listing{idx}\"\n",
        "    with open(os.path.join(\"real-estate-eval-5\", listing, \"listing-text.txt\")) as f:\n",
        "        texts.append(f.read())\n",
        "    for idx, img_name in enumerate([\"img1.png\", \"img2.png\", \"img3.png\"]):\n",
        "        path = os.path.join(\"real-estate-eval-5\", listing, img_name)\n",
        "        img = Image.open(path)\n",
        "        img_arr = np.asarray(img)\n",
        "        if idx == 0:\n",
        "            fst_imgs.append(img_arr)\n",
        "        elif idx == 1:\n",
        "            snd_imgs.append(img_arr)\n",
        "        elif idx == 2:\n",
        "            thrd_imgs.append(img_arr)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    fst_img_blocks, snd_img_blocks, thrd_img_blocks, text_blocks = [], [], [], []\n",
        "    for fst_img, snd_img, thrd_img, text in zip(fst_imgs, snd_imgs, thrd_imgs, texts):\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column():\n",
        "                fst_img_blocks.append(gr.Image(value=fst_img))\n",
        "            with gr.Column():\n",
        "                snd_img_blocks.append(gr.Image(value=snd_img))\n",
        "            with gr.Column():\n",
        "                thrd_img_blocks.append(gr.Image(value=thrd_img))\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text_blocks.append(gr.Textbox(value=text, info=\"Text Description\"))\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg4yRYx26ecr"
      },
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpNe3bFk6FMD"
      },
      "source": [
        "As a first step, we need to write a custom `pz.DataReader` to enable PZ to load our data properly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FcqpZySyWbl"
      },
      "outputs": [],
      "source": [
        "from palimpzest.core.lib.fields import ImageFilepathField, ListField\n",
        "\n",
        "# we first define the schema for each record output by the DataReader\n",
        "real_estate_listing_cols = [\n",
        "    {\"name\": \"listing\", \"type\": str, \"desc\": \"The name of the listing\"},\n",
        "    {\"name\": \"text_content\", \"type\": str, \"desc\": \"The content of the listing's text description\"},\n",
        "    {\"name\": \"image_filepaths\", \"type\": ListField(ImageFilepathField), \"desc\": \"A list of the filepaths for each image of the listing\"},\n",
        "]\n",
        "\n",
        "# we then implement the DataReader\n",
        "class RealEstateListingReader(pz.DataReader):\n",
        "    def __init__(self, listings_dir):\n",
        "        super().__init__(schema=real_estate_listing_cols)\n",
        "        self.listings_dir = listings_dir\n",
        "        self.listings = sorted(os.listdir(self.listings_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.listings)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # get listing\n",
        "        listing = self.listings[idx]\n",
        "\n",
        "        # get fields\n",
        "        image_filepaths, text_content = [], None\n",
        "        listing_dir = os.path.join(self.listings_dir, listing)\n",
        "        for file in os.listdir(listing_dir):\n",
        "            if file.endswith(\".txt\"):\n",
        "                with open(os.path.join(listing_dir, file), \"rb\") as f:\n",
        "                    text_content = f.read().decode(\"utf-8\")\n",
        "            elif file.endswith(\".png\"):\n",
        "                image_filepaths.append(os.path.join(listing_dir, file))\n",
        "\n",
        "        # construct and return dictionary with fields\n",
        "        return {\"listing\": listing, \"text_content\": text_content, \"image_filepaths\": image_filepaths}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q45qFYcM0N9w"
      },
      "source": [
        "Every `pz.DataReader` must have the following:\n",
        "1. A `schema` defining the fields present in each output record\n",
        "2. A `__len__()` function which returns the number of items in the dataset\n",
        "3. A `__getitem__(idx)` function which returns the `idx`th item in the dataset\n",
        "\n",
        "Once we've implemented the `pz.DataReader`, we can compose our PZ program as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qOI9WOY0X4_"
      },
      "outputs": [],
      "source": [
        "# schema for computing the address and price of each home\n",
        "real_estate_text_cols = [\n",
        "    {\"name\": \"address\", \"type\": str, \"desc\": \"The address of the property\"},\n",
        "    {\"name\": \"price\", \"type\": int | float, \"desc\": \"The listed price of the property\"},\n",
        "]\n",
        "\n",
        "# define a UDF for filtering based on a price range\n",
        "def in_price_range(record: dict):\n",
        "    try:\n",
        "        price = record[\"price\"]\n",
        "        if isinstance(price, str):\n",
        "            price = price.strip()\n",
        "            price = int(price.replace(\"$\", \"\").replace(\",\", \"\"))\n",
        "        return 6e5 < price <= 2e6\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# construct our PZ program to filter for listings matching our search criteria\n",
        "ds = pz.Dataset(RealEstateListingReader(\"real-estate-eval-5\"))\n",
        "ds = ds.sem_add_columns(real_estate_text_cols, depends_on=\"text_content\")\n",
        "ds = ds.sem_filter(\n",
        "    \"The interior is modern and attractive, and has lots of natural sunlight\",\n",
        "    depends_on=\"image_filepaths\",\n",
        ")\n",
        "ds = ds.filter(in_price_range, depends_on=\"price\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eitGXQCS0YRY"
      },
      "source": [
        "First, we write a schema for the `address` and `price` fields we wish to compute.\n",
        "\n",
        "Next, we write a UDF to filter for homes based on our price range.\n",
        "\n",
        "Then we compose our program by:\n",
        "1. Constructing our `pz.DataReader` with the real estate data\n",
        "2. Using `sem_add_columns()` to compute the `address` and `price`\n",
        "3. Using a `sem_filter()` to filter for modern homes with lots of sunlight\n",
        "4. Using our UDF to filter for homes based on our price range\n",
        "\n",
        "We now execute the program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFmakjcQ4W5o"
      },
      "outputs": [],
      "source": [
        "# execute the computation w/the MaxQuality policy\n",
        "config = pz.QueryProcessorConfig(policy=pz.MaxQuality(), execution_strategy=\"parallel\", progress=True)\n",
        "output = ds.run(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHesFcjc4snL"
      },
      "source": [
        "Now let's take a look at our output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAxeR_R54vuY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "demo.close()\n",
        "\n",
        "# Boilerplate code to build our visualization\n",
        "fst_imgs, snd_imgs, thrd_imgs, addrs, prices = [], [], [], [], []\n",
        "for record in output:\n",
        "    addrs.append(record.address)\n",
        "    prices.append(record.price)\n",
        "    for idx, img_name in enumerate([\"img1.png\", \"img2.png\", \"img3.png\"]):\n",
        "        path = os.path.join(\"real-estate-eval-5\", record.listing, img_name)\n",
        "        img = Image.open(path)\n",
        "        img_arr = np.asarray(img)\n",
        "        if idx == 0:\n",
        "            fst_imgs.append(img_arr)\n",
        "        elif idx == 1:\n",
        "            snd_imgs.append(img_arr)\n",
        "        elif idx == 2:\n",
        "            thrd_imgs.append(img_arr)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    fst_img_blocks, snd_img_blocks, thrd_img_blocks, addr_blocks, price_blocks = [], [], [], [], []\n",
        "    for fst_img, snd_img, thrd_img, addr, price in zip(fst_imgs, snd_imgs, thrd_imgs, addrs, prices):\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column():\n",
        "                fst_img_blocks.append(gr.Image(value=fst_img))\n",
        "            with gr.Column():\n",
        "                snd_img_blocks.append(gr.Image(value=snd_img))\n",
        "            with gr.Column():\n",
        "                thrd_img_blocks.append(gr.Image(value=thrd_img))\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                addr_blocks.append(gr.Textbox(value=addr, info=\"Address\"))\n",
        "            with gr.Column():\n",
        "                price_blocks.append(gr.Textbox(value=price, info=\"Price\"))\n",
        "\n",
        "    plan_str = list(output.execution_stats.plan_strs.values())[0]\n",
        "    gr.Textbox(value=plan_str, info=\"Query Plan\")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpCo8uF54sPi"
      },
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAgG4wBKyZM0"
      },
      "source": [
        "# Third PZ Program: Optimizing a Biomedical Classification Pipeline\n",
        "\n",
        "For our final demo, we will work with a subset of the BioDEX dataset.\n",
        "\n",
        "Each input in the dataset is a medical report describing an adverse reaction a patient had in response to taking one or more drugs.\n",
        "\n",
        "The goal is to correctly predict the reactions experienced by the patient by matching them to a database of ~24,300 official medical reaction terms.\n",
        "\n",
        "We are going to use Palimpzest to implement the following pipeline:\n",
        "1. Load a medical report\n",
        "2. Compute a list of reactions mentioned in the report\n",
        "3. Retrieve the most similar reaction terms from a vector database with embeddings for each of the ~24,300 official terms\n",
        "4. Re-rank the list of official terms based on their relevance\n",
        "\n",
        "First, we will once again create a `pz.DataReader` to load the medical reports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CeZ9Ib1yY1n"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from functools import partial\n",
        "\n",
        "# define the schema for records returned by the DataReader\n",
        "biodex_entry_cols = [\n",
        "    {\"name\": \"pmid\", \"type\": str, \"desc\": \"The PubMed ID of the medical paper\"},\n",
        "    {\"name\": \"title\", \"type\": str, \"desc\": \"The title of the medical paper\"},\n",
        "    {\"name\": \"abstract\", \"type\": str, \"desc\": \"The abstract of the medical paper\"},\n",
        "    {\"name\": \"fulltext\", \"type\": str, \"desc\": \"The full text of the medical paper, which contains information relevant for creating a drug safety report.\"},\n",
        "]\n",
        "\n",
        "# implement the DataReader\n",
        "class BiodexReader(pz.DataReader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        rp_at_k: int = 5,\n",
        "        num_samples: int = 10,\n",
        "        split: str = \"test\",\n",
        "        shuffle: bool = True,\n",
        "        seed: int = 42,\n",
        "    ):\n",
        "        super().__init__(biodex_entry_cols)\n",
        "\n",
        "        self.dataset = datasets.load_dataset(\"BioDEX/BioDEX-Reactions\", split=split).to_pandas()\n",
        "        if shuffle:\n",
        "            self.dataset = self.dataset.sample(n=num_samples, random_state=seed).to_dict(orient=\"records\")\n",
        "        else:\n",
        "            self.dataset = self.dataset.to_dict(orient=\"records\")[:num_samples]\n",
        "\n",
        "        self.rp_at_k = rp_at_k\n",
        "        self.num_samples = num_samples\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.split = split\n",
        "\n",
        "    def compute_label(self, entry: dict) -> dict:\n",
        "        \"\"\"Compute the label for a BioDEX report given its entry in the dataset.\"\"\"\n",
        "        reactions_lst = [\n",
        "            reaction.strip().lower().replace(\"'\", \"\").replace(\"^\", \"\")\n",
        "            for reaction in entry[\"reactions\"].split(\",\")\n",
        "        ]\n",
        "        label_dict = {\n",
        "            \"reactions\": reactions_lst,\n",
        "            \"reaction_labels\": reactions_lst,\n",
        "            \"ranked_reaction_labels\": reactions_lst,\n",
        "        }\n",
        "        return label_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def rank_precision_at_k(preds, targets, k: int):\n",
        "        if preds is None:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            # lower-case each list\n",
        "            preds = [pred.strip().lower().replace(\"'\", \"\").replace(\"^\", \"\") for pred in preds]\n",
        "            targets = set([target.strip().lower().replace(\"'\", \"\").replace(\"^\", \"\") for target in targets])\n",
        "\n",
        "            # compute rank-precision at k\n",
        "            rn = len(targets)\n",
        "            denom = min(k, rn)\n",
        "            total = 0.0\n",
        "            for i in range(k):\n",
        "                total += preds[i] in targets if i < len(preds) else 0.0\n",
        "\n",
        "            return total / denom\n",
        "\n",
        "        except Exception:\n",
        "            return 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def term_recall(preds, targets):\n",
        "        if preds is None:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            # normalize terms in each list\n",
        "            pred_terms = set([\n",
        "                term.strip()\n",
        "                for pred in preds\n",
        "                for term in pred.lower().replace(\"'\", \"\").replace(\"^\", \"\").split(\" \")\n",
        "            ])\n",
        "            target_terms = ([\n",
        "                term.strip()\n",
        "                for target in targets\n",
        "                for term in target.lower().replace(\"'\", \"\").replace(\"^\", \"\").split(\" \")\n",
        "            ])\n",
        "\n",
        "            # compute term recall and return\n",
        "            intersect = pred_terms.intersection(target_terms)\n",
        "            term_recall = len(intersect) / len(target_terms)\n",
        "\n",
        "            return term_recall\n",
        "\n",
        "        except Exception:\n",
        "            return 0.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # get entry\n",
        "        entry = self.dataset[idx]\n",
        "\n",
        "        # get input fields\n",
        "        pmid = entry[\"pmid\"]\n",
        "        title = entry[\"title\"]\n",
        "        abstract = entry[\"abstract\"]\n",
        "        fulltext = entry[\"fulltext\"]\n",
        "\n",
        "        # create item with fields\n",
        "        item = {\"fields\": {}, \"labels\": {}, \"score_fn\": {}}\n",
        "        item[\"fields\"][\"pmid\"] = pmid\n",
        "        item[\"fields\"][\"title\"] = title\n",
        "        item[\"fields\"][\"abstract\"] = abstract\n",
        "        item[\"fields\"][\"fulltext\"] = fulltext\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            # add label info\n",
        "            item[\"labels\"] = self.compute_label(entry)\n",
        "\n",
        "            # add scoring functions for list fields\n",
        "            rank_precision_at_k = partial(BiodexReader.rank_precision_at_k, k=self.rp_at_k)\n",
        "            item[\"score_fn\"][\"reactions\"] = BiodexReader.term_recall\n",
        "            item[\"score_fn\"][\"reaction_labels\"] = BiodexReader.term_recall\n",
        "            item[\"score_fn\"][\"ranked_reaction_labels\"] = rank_precision_at_k\n",
        "\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyr71cs3EQpl"
      },
      "source": [
        "There are a few new features of this `pz.DataReader` which are needed for the optimization process:\n",
        "1. `__getitem__()` returns a dictionary with top-level keys `{\"fields\", \"labels\", \"score_fn\"}`\n",
        "2. `fields` contains the data emitted by the `pz.DataReader`\n",
        "3. (for `train` data only): `labels` contains the expected results for each output field\n",
        "4. (for `train` data only): `score_fn` contains scoring functions for each output field\n",
        "\n",
        "Once we've defined our `pz.DataReader`, we can create our training and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHYqdDrlG8zP"
      },
      "outputs": [],
      "source": [
        "SEED = 123\n",
        "\n",
        "# create train and test datasets; and validator\n",
        "train_datareader = BiodexReader(split=\"train\", seed=SEED)\n",
        "test_datareader = BiodexReader(split=\"test\", num_samples=20, seed=SEED)\n",
        "validator = pz.Validator(train_datareader, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHaVbQHiG-Rf"
      },
      "source": [
        "We now implement the logic for the `sem_topk` operator for you. It fetches the five most similar medical terms for each reaction computed by PZ, sorts them based on similarity, and then returns the final top-k most similar terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE3scuaXI6HB"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils.embedding_functions.openai_embedding_function import OpenAIEmbeddingFunction\n",
        "\n",
        "# load index [text-embedding-3-small]\n",
        "chroma_client = chromadb.PersistentClient(\".chroma-biodex\")\n",
        "openai_ef = OpenAIEmbeddingFunction(\n",
        "  api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "  model_name=\"text-embedding-3-small\",\n",
        ")\n",
        "index = chroma_client.get_collection(\"biodex-reaction-terms\", embedding_function=openai_ef)\n",
        "\n",
        "def search_func(index: chromadb.Collection, query: list[list[float]], k: int) -> list[str]:\n",
        "    # execute query with embeddings\n",
        "    results = index.query(query, n_results=5)\n",
        "\n",
        "    # get list of result terms with their cosine similarity scores\n",
        "    final_results = []\n",
        "    for query_docs, query_distances in zip(results[\"documents\"], results[\"distances\"]):\n",
        "        for doc, dist in zip(query_docs, query_distances):\n",
        "            cosine_similarity = 1 - dist\n",
        "            final_results.append({\"content\": doc, \"similarity\": cosine_similarity})\n",
        "\n",
        "    # sort the results by similarity score\n",
        "    sorted_results = sorted(final_results, key=lambda result: result[\"similarity\"], reverse=True)\n",
        "\n",
        "    # remove duplicates\n",
        "    sorted_results_set = set()\n",
        "    final_sorted_results = []\n",
        "    for result in sorted_results:\n",
        "        if result[\"content\"] not in sorted_results_set:\n",
        "            sorted_results_set.add(result[\"content\"])\n",
        "            final_sorted_results.append(result[\"content\"])\n",
        "\n",
        "    # return the top-k similar results and generation stats\n",
        "    return {\"reaction_labels\": final_sorted_results[:k]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCYi92YCJSuq"
      },
      "source": [
        "Finally, we can construct our PZ program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3UkmjQFBSjc"
      },
      "outputs": [],
      "source": [
        "# define the schema for each computation in our program\n",
        "biodex_reactions_cols = [\n",
        "    {\"name\": \"reactions\", \"type\": list[str], \"desc\": \"The list of all medical conditions experienced by the patient as discussed in the report. Try to provide as many relevant medical conditions as possible.\"},\n",
        "]\n",
        "biodex_reaction_labels_cols = [\n",
        "    {\"name\": \"reaction_labels\", \"type\": list[str], \"desc\": \"Official terms for medical conditions listed in `reactions`\"},\n",
        "]\n",
        "biodex_ranked_reactions_labels_cols = [\n",
        "    {\"name\": \"ranked_reaction_labels\", \"type\": list[str], \"desc\": \"The ranked list of medical conditions experienced by the patient. The most relevant label occurs first in the list. Be sure to rank ALL of the inputs.\"},\n",
        "]\n",
        "\n",
        "\n",
        "# construct pz plan\n",
        "plan = pz.Dataset(test_datareader)\n",
        "plan = plan.sem_add_columns(biodex_reactions_cols)\n",
        "plan = plan.sem_topk(\n",
        "    index=index,\n",
        "    search_func=search_func,\n",
        "    search_attr=\"reactions\",\n",
        "    output_attrs=biodex_reaction_labels_cols,\n",
        ")\n",
        "plan = plan.sem_add_columns(biodex_ranked_reactions_labels_cols, depends_on=[\"title\", \"abstract\", \"fulltext\", \"reaction_labels\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvFZO-LKKq3F"
      },
      "source": [
        "First, let's execute our plan without training data and score our performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khhJJUXTLNDz"
      },
      "outputs": [],
      "source": [
        "def score_output(output, seed):\n",
        "    # score output\n",
        "    test_dataset = datasets.load_dataset(\"BioDEX/BioDEX-Reactions\", split=\"test\").to_pandas()\n",
        "    test_dataset = test_dataset.sample(n=20, random_state=seed).to_dict(orient=\"records\")\n",
        "\n",
        "    # construct mapping from pmid --> label (field, value) pairs\n",
        "    def compute_target_record(entry):\n",
        "        reactions_lst = [\n",
        "            reaction.strip().lower().replace(\"'\", \"\").replace(\"^\", \"\")\n",
        "            for reaction in entry[\"reactions\"].split(\",\")\n",
        "        ]\n",
        "        label_dict = {\"ranked_reaction_labels\": reactions_lst}\n",
        "        return label_dict\n",
        "\n",
        "    label_fields_to_values = {\n",
        "        entry[\"pmid\"]: compute_target_record(entry) for entry in test_dataset\n",
        "    }\n",
        "\n",
        "    def rank_precision_at_k(preds: list, targets: list, k: int):\n",
        "        if preds is None:\n",
        "            return 0.0\n",
        "\n",
        "        # lower-case each list\n",
        "        preds = [pred.lower().replace(\"'\", \"\").replace(\"^\", \"\") for pred in preds]\n",
        "        targets = set([target.lower().replace(\"'\", \"\").replace(\"^\", \"\") for target in targets])\n",
        "\n",
        "        # compute rank-precision at k\n",
        "        rn = len(targets)\n",
        "        denom = min(k, rn)\n",
        "        total = 0.0\n",
        "        for i in range(k):\n",
        "            total += preds[i] in targets if i < len(preds) else 0.0\n",
        "\n",
        "        return total / denom\n",
        "\n",
        "    def compute_avg_rp_at_k(records, k=5):\n",
        "        total_rp_at_k = 0\n",
        "        bad = 0\n",
        "        for record in records:\n",
        "            pmid = record['pmid']\n",
        "            preds = record['ranked_reaction_labels']\n",
        "            targets = label_fields_to_values[pmid]['ranked_reaction_labels']\n",
        "            try:\n",
        "                total_rp_at_k += rank_precision_at_k(preds, targets, k)\n",
        "            except Exception:\n",
        "                bad += 1\n",
        "\n",
        "        return total_rp_at_k / len(records), bad\n",
        "\n",
        "    rp_at_k, bad = compute_avg_rp_at_k([record.to_dict() for record in output], k=5)\n",
        "    final_plan_id = list(output.execution_stats.plan_stats.keys())[0]\n",
        "    final_plan_str = output.execution_stats.plan_strs[final_plan_id]\n",
        "    print(\"---\")\n",
        "    print(\"#########################\")\n",
        "    print(f\"##### RP@5: {rp_at_k:.5f} #####\")\n",
        "    print(\"#########################\")\n",
        "    print(\"---\")\n",
        "    print(f\"Optimization time: {output.execution_stats.optimization_time:.2f}s\")\n",
        "    print(f\"Optimization cost: ${output.execution_stats.optimization_cost:.3f}\")\n",
        "    print(\"---\")\n",
        "    print(f\"Plan exec. time: {output.execution_stats.plan_execution_time:.2f}s\")\n",
        "    print(f\"Plan exec. cost: ${output.execution_stats.plan_execution_cost:.3f}\")\n",
        "    print(\"---\")\n",
        "    print(f\"Total time: {output.execution_stats.total_execution_time:.2f}s\")\n",
        "    print(f\"Total Cost: ${output.execution_stats.total_execution_cost:.3f}\")\n",
        "    print(\"---\")\n",
        "    print(\"Final Plan:\")\n",
        "    print(final_plan_str)\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.disabled = True\n",
        "\n",
        "# execute pz plan\n",
        "config = pz.QueryProcessorConfig(\n",
        "    policy=pz.MaxQuality(),\n",
        "    execution_strategy=\"parallel\",\n",
        "    max_workers=64,\n",
        "    progress=True,\n",
        ")\n",
        "\n",
        "output = plan.run(config=config, seed=SEED)\n",
        "score_output(output, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORUDS35-K1Nf"
      },
      "source": [
        "Now, let's run the program again while using our `train_datareader` as a validation dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4htdaTDKpFc"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.disabled = True\n",
        "\n",
        "# execute pz plan\n",
        "config = pz.QueryProcessorConfig(\n",
        "    policy=pz.MaxQuality(),\n",
        "    validator=validator,\n",
        "    optimizer_strategy=\"pareto\",\n",
        "    sentinel_execution_strategy=\"mab\",\n",
        "    execution_strategy=\"parallel\",\n",
        "    use_final_op_quality=True,\n",
        "    max_workers=64,\n",
        "    progress=True,\n",
        ")\n",
        "\n",
        "output = plan.run(config=config, k=6, j=4, sample_budget=72, seed=SEED)\n",
        "score_output(output, seed=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT3iZr6-Kpab"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
