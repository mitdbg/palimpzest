{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickstart\n",
    "This notebook contains a sample program to guide you through the features of the Palimpzest (PZ) library. \n",
    "PZ provides a high-level, declarative interface for composing and executing pipelines of semantic operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "As Palimpzest is accessing LLM models, you need to set up **at least** one of the following\n",
    "API keys as environment variables:\n",
    "\n",
    "- `OPENAI_API_KEY` for using OPENAI's GPT-3.5 and GPT-4 models\n",
    "- `TOGETHER_API_KEY` for using TogetherAI's LLM models, including Mixtral\n",
    "\n",
    "Support for local model execution and more APIs is underway!\n",
    "\n",
    "Edit the following snippet with your API key in order to run the notebook.\n",
    "You don't need to run this cell if you have already set one of the keys in the corresponding environment variable.\n",
    "You can provide one, two, or all three keys in the snippet below. The more keys, the more optimizations Palimpzest will be able to perform!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "# os.environ[\"TOGETHER_API_KEY\"] = \"your-together-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Use Case: Enron Email Dataset\n",
    "In this application use case, we will work with the Enron Email Dataset. The Enron Email Dataset is a large database of over 600,000 emails. Don't worry! For this demo, we will only be working with a small subset of the dataset.\n",
    "\n",
    "In this demo, we are going to use Palimpzest to perform the following tasks:\n",
    "1. Load the text files that contain the emails. Each `.txt` file contains a single email.\n",
    "2. We will use Palimpzest to convert the textual files into an \"Email\" type. This will mean extracting explicitly the sender, subject, and date of each email.\n",
    "3. Then, we will use Palimpzest to filter the emails to only retain the ones that mention a vacation plan and were sent in the month of July.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset\n",
    "\n",
    "First, we have to load the directory containing the textual files in Palimpzest. To do so, we use the `register_dataset` function from the `datamanager` module. This function takes the path to the directory and a name which can be later used to reference this dataset.\n",
    "\n",
    "This step has to be run once for each dataset you want to load, and this information will be persisted on disk. Therefore if you have already loaded the dataset, you can skip this step.\n",
    "\n",
    "As you can see, when we load the dataset, we can specify a schema for the input objects we are going to work with. \n",
    "A schema is comprised of a set of attributes that Palimpzest will extract from the input objects.\n",
    "\n",
    "In this case, we know the content of `enron-tiny` is textual files, so we can specify the schema type `TextFile`. This built-in schema is used to parse the textual content of the files which will be saved in the `content` attribute.\n",
    "Palimpzest will automatically detect the file format and the number of files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import palimpzest.datamanager.datamanager as pzdm\n",
    "from palimpzest.core.lib.schemas import TextFile\n",
    "from palimpzest.sets import Dataset\n",
    "\n",
    "# Dataset registration\n",
    "dataset_path = \"testdata/enron-tiny\"\n",
    "dataset_name = \"enron-tiny\"\n",
    "pzdm.DataDirectory().register_local_directory(dataset_path, dataset_name)\n",
    "\n",
    "# Dataset loading\n",
    "dataset = Dataset(dataset_name, schema=TextFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Convert the textual files into an \"Email\" type\n",
    "Since we want to extract useful information from the input files, we need to define a custom `Schema` to specify which attributes we are interested in.\n",
    "Fear not! This is a simple process. We just need to define a class that inherits from `Schema` and specify the attributes we want to extract, using descriptive names and natural language descriptions.\n",
    "\n",
    "Do not forget to include a class description, as this will be used by Palimpzest during the conversion process!\n",
    "\n",
    "The `Email` schema will extract the sender, subject, and date of the email. We will use this schema when calling the `dataset.convert(output_schema)` function, which will signal to Palimpzest that we want to convert files with a certain input schema into a given output schema (by extracting the necessary attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palimpzest.core.lib.fields import Field\n",
    "from palimpzest.core.lib.schemas import Schema\n",
    "\n",
    "\n",
    "class Email(Schema):\n",
    "    \"\"\"Represents an email, which in practice is usually from a text file\"\"\"\n",
    "    sender = Field(desc=\"The email address of the sender\")\n",
    "    subject = Field(desc=\"The subject of the email\")\n",
    "    date = Field(desc=\"The date the email was sent\")\n",
    "\n",
    "dataset = dataset.convert(Email, desc=\"An email from the Enron dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the dataset, you will see that it now has a schema of Email. \n",
    "However, the schema is not yet applied to the files themselves and the attributes are not yet extracted.\n",
    "This is by design: first, users define all of the operations they want to perform on the dataset, and then they invoke the execution of these operations.\n",
    "\n",
    "Thanks to this design, Palimpzest can optimize the execution of the operations and also avoid unnecessary computations, for example if it recognizes that some of the later computation does not depend on previous steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dataset(schema=<class '__main__.Email'>, desc=An email from the Enron dataset, filter=None, udf=None, agg_func=None, limit=None, project_cols=None, uid=06a23b1a60)\n",
      "The schema of the dataset is <class '__main__.Email'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset\", dataset)\n",
    "print(\"The schema of the dataset is\", dataset.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply a Filter to the Emails\n",
    "Now that we have the emails in the dataset, we can filter them to only retain the ones that mention a vacation plan and were sent in the month of July.\n",
    "\n",
    "To do this, we will use the `filter` function. This function takes a string which describes in natural language which condition we want the records to satisfy to pass the filter.\n",
    "\n",
    "When using natural language, you don't need to worry about implementing the filter itself, but the computation will be performed by LLM models. Such is the power of Palimpzest! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(\"The email was sent in July\")\n",
    "dataset = dataset.filter(\"The email is about holidays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the operations\n",
    "Finally, we can execute the operations we have defined on the dataset by calling the `Execute` function on the final dataset. \n",
    "There is one important parameter to discuss here: an execution `policy`. This parameter allows you to specify how the operations should be executed.\n",
    "Palimpzest optimizes along three axes: cost, time, and quality of the output. You can specify which of these axes is most important to you, and Palimpzest will optimize the execution accordingly.\n",
    "\n",
    "Here, we use the `MinCost` policy, which will try to minimize the cost of the execution regardless of output quality and runtime. This is useful for large datasets or when you are experimenting with the framework and want to keep the costs low.\n",
    "You can experiment with the `MaxQuality` policy to see how it affects the execution of the operations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palimpzest.policy import MaxQuality, MinCost\n",
    "from palimpzest.query.processor.config import QueryProcessorConfig\n",
    "\n",
    "policy = MinCost()\n",
    "config = QueryProcessorConfig(\n",
    "    policy=policy,\n",
    "    verbose=True,\n",
    "    processing_strategy=\"no_sentinel\",\n",
    "    execution_strategy=\"sequential\",\n",
    "    optimizer_strategy=\"pareto\",\n",
    ")\n",
    "data_record_collection = dataset.run(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the output\n",
    "\n",
    "The output of our data pipeline can be found in the `results` variable. \n",
    "To print the results as a table, we will initialize a pandas dataframe using the `to_dict` method of the output objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_df = data_record_collection.to_df(project_cols=[\"date\", \"sender\", \"subject\"])\n",
    "display(output_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, that is not the only output of the pipeline execution! \n",
    "\n",
    "Palimpzest also provides a detailed report of the execution, with statistics about the runtime and cost of each operation.\n",
    "To access these statistics, you can use the `execution_stats` attribute returned by the call to `dataset.run(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_stats = data_record_collection.execution_stats\n",
    "print(\"Time to find an optimal plan:\", execution_stats.total_optimization_time,\"s\")\n",
    "print(\"Time to execute the plan:\", execution_stats.total_execution_time, \"s\")\n",
    "print(\"Total cost:\", execution_stats.total_execution_cost, \"USD\")\n",
    "\n",
    "print(\"Final plan executed:\")\n",
    "for plan, stats in execution_stats.plan_stats.items():\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope this notebook is only the start of your Palimpzest journey! Feel free to reach out to us for more information!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
