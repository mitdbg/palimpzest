{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f3ca15-6c8b-4a6a-b6c4-6d5ecdd7030f",
   "metadata": {},
   "source": [
    "## Define Plan and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94229b85-d9b0-4ec2-a366-66cd73631694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from palimpzest.datamanager import DataDirectory\n",
    "from palimpzest.corelib import Schema\n",
    "from ragatouille import RAGPretrainedModel\n",
    "import palimpzest as pz\n",
    "import datasets\n",
    "\n",
    "class BiodexEntry(Schema):\n",
    "    \"\"\"A single entry in the Biodex ICSR Dataset.\"\"\"\n",
    "\n",
    "    pmid = pz.StringField(desc=\"The PubMed ID of the medical paper\", required=True)\n",
    "    title = pz.StringField(desc=\"The title of the medical paper\", required=True)\n",
    "    abstract = pz.StringField(desc=\"The abstract of the medical paper\", required=True)\n",
    "    fulltext = pz.StringField(desc=\"The full text of the medical paper, which contains information relevant for creating a drug safety report.\", required=True)\n",
    "\n",
    "class BiodexReactions(BiodexEntry):\n",
    "    \"\"\"\n",
    "    You will be presented with the text of a medical article which is partially or entirely about\n",
    "    an adverse event experienced by a patient in response to taking one or more drugs. In this task,\n",
    "    you will be asked to extract a list of the primary adverse reactions which are experienced by the patient.\n",
    "    \"\"\"\n",
    "    # reactions = pz.ListField(desc=\"The **list** of the adverse reaction(s) which may have resulted from taking the drug(s) discussed in the report.\\n - For example: [\\\"Epstein-Barr virus\\\", \\\"infection reactivation\\\", \\\"Idiopathic interstitial pneumonia\\\"]\", element_type=pz.StringField, required=True)\n",
    "    reactions = pz.ListField(desc=\"The list of the adverse reaction term(s) which may have resulted from taking the drug(s) discussed in the report.\", element_type=pz.StringField, required=True)\n",
    "\n",
    "class BiodexReactionLabels(BiodexReactions):\n",
    "    \"\"\"\n",
    "    Retrieve the labels which are most relevant for the given set of inferred reactions.\n",
    "    \"\"\"\n",
    "    reaction_labels = pz.ListField(desc=\"Most relevant official terms for adverse reactions for the provided `reactions`\",\n",
    "                                   element_type=pz.StringField, required=True)\n",
    "\n",
    "class BiodexRankedReactions(BiodexReactionLabels):\n",
    "    \"\"\"\n",
    "    You will be presented with the text of a medical article which is partially or entirely about\n",
    "    an adverse event experienced by a patient in response to taking one or more drugs. You will also\n",
    "    be presented with a list of inferred reactions, and a set of retrieved labels which were matched\n",
    "    to these inferred reactions. In this task, you are asked to output a ranked list of the labels\n",
    "    which are most applicable based on the context of the article. Your output list must:\n",
    "    - contain only elements from `reaction_labels`\n",
    "    - place the most likely label first and the least likely label last\n",
    "    - you may omit labels if you think they do not describe a reaction experienced by the patient\n",
    "    \"\"\"\n",
    "    ranked_reaction_labels = pz.ListField(desc=\"The ranked list of labels for adverse reactions experienced by the patient. The most likely label occurs first in the list.\",\n",
    "                                          element_type=pz.StringField, required=True)\n",
    "\n",
    "\n",
    "class BiodexValidationSource(pz.ValidationDataSource):\n",
    "    def __init__(self, datasetId, reactions_only: bool=True, rp_at_k: int=5, num_samples: int=5, shuffle: bool=False, seed: int=42):\n",
    "        super().__init__(BiodexEntry, datasetId)\n",
    "        self.dataset = datasets.load_dataset(\"BioDEX/BioDEX-ICSR\")\n",
    "        self.train_dataset = [self.dataset['train'][idx] for idx in range(100)]\n",
    "        self.test_dataset = [self.dataset['train'][idx] for idx in range(100, 150)]\n",
    "\n",
    "        # sample from full test dataset\n",
    "        # self.test_dataset = [self.dataset['test'][idx] for idx in range(len(self.dataset['test']))]\n",
    "        # self.test_dataset = self.test_dataset[:250] # use first 250 to compare directly with biodex\n",
    "\n",
    "        self.reactions_only = reactions_only\n",
    "        self.rp_at_k = rp_at_k\n",
    "        self.num_samples = num_samples\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "        # construct mapping from listing --> label (field, value) pairs\n",
    "        def compute_target_record(entry, reactions_only: bool=False):\n",
    "            target_lst = entry['target'].split('\\n')\n",
    "            label_dict = {\n",
    "                \"serious\": int(target_lst[0].split(':')[-1]),\n",
    "                \"patientsex\": int(target_lst[1].split(':')[-1]),\n",
    "                \"drugs\": [drug.strip().lower() for drug in target_lst[2].split(':')[-1].split(\",\")],\n",
    "                \"reactions\": [reaction.strip().lower() for reaction in target_lst[3].split(':')[-1].split(\",\")],\n",
    "                \"reaction_labels\": [reaction.strip().lower() for reaction in target_lst[3].split(':')[-1].split(\",\")],\n",
    "                \"ranked_reaction_labels\": [reaction.strip().lower() for reaction in target_lst[3].split(':')[-1].split(\",\")],\n",
    "            }\n",
    "            if reactions_only:\n",
    "                label_dict = {\n",
    "                    k: v\n",
    "                    for k, v in label_dict.items()\n",
    "                    if k in [\"reactions\", \"reaction_labels\", \"ranked_reaction_labels\"]\n",
    "                }\n",
    "            return label_dict\n",
    "\n",
    "        self.label_fields_to_values = {\n",
    "            entry['pmid']: compute_target_record(entry, reactions_only=reactions_only)\n",
    "            for entry in self.test_dataset\n",
    "        }\n",
    "\n",
    "        # shuffle records if shuffle = True\n",
    "        if shuffle:\n",
    "            random.Random(seed).shuffle(self.train_dataset)\n",
    "\n",
    "        # trim to number of samples\n",
    "        self.train_dataset = self.train_dataset[:num_samples]\n",
    "\n",
    "    def copy(self):\n",
    "        return BiodexValidationSource(self.dataset_id, self.num_samples, self.shuffle, self.seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_dataset)\n",
    "\n",
    "    def getValLength(self):\n",
    "        return len(self.train_dataset)\n",
    "\n",
    "    def getSize(self):\n",
    "        return 0\n",
    "\n",
    "    def getFieldToMetricFn(self):\n",
    "        # define f1 function\n",
    "        def f1_eval(preds: list, targets: list):\n",
    "            if preds is None:\n",
    "                return 0.0\n",
    "\n",
    "            try:\n",
    "                # compute precision and recall\n",
    "                s_preds = set([pred.lower() for pred in preds])\n",
    "                s_targets = set([target.lower() for target in targets])\n",
    "\n",
    "                intersect = s_preds.intersection(s_targets)\n",
    "\n",
    "                precision = len(intersect) / len(s_preds) if len(s_preds) > 0 else 0.0\n",
    "                recall = len(intersect) / len(s_targets)\n",
    "\n",
    "                # compute f1 score and return\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "                return f1\n",
    "\n",
    "            except:\n",
    "                os.makedirs(\"f1-errors\", exist_ok=True)\n",
    "                ts = time.time()\n",
    "                with open(f\"f1-errors/error-{ts}.txt\", \"w\") as f:\n",
    "                    f.write(str(preds))\n",
    "                return 0.0\n",
    "\n",
    "        # define rank precision at k\n",
    "        def rank_precision_at_k(preds: list, targets: list):\n",
    "            if preds is None:\n",
    "                return 0.0\n",
    "\n",
    "            try:\n",
    "                # lower-case each list\n",
    "                preds = [pred.lower() for pred in preds]\n",
    "                targets = set([target.lower() for target in targets])\n",
    "\n",
    "                # compute rank-precision at k\n",
    "                Rn = len(targets)\n",
    "                denom = min(self.rp_at_k, Rn)\n",
    "                total = 0.0\n",
    "                for i in range(self.rp_at_k):\n",
    "                    total += preds[i] in targets if i < len(preds) else 0.0\n",
    "\n",
    "                return total / denom\n",
    "\n",
    "            except:\n",
    "                os.makedirs(\"rp@k-errors\", exist_ok=True)\n",
    "                ts = time.time()\n",
    "                with open(f\"rp@k-errors/error-{ts}.txt\", \"w\") as f:\n",
    "                    f.write(str(preds))\n",
    "                return 0.0\n",
    "\n",
    "        # define quality eval function for drugs and reactions fields\n",
    "        fields_to_metric_fn = {}\n",
    "        if self.reactions_only:\n",
    "            fields_to_metric_fn = {\n",
    "                \"reactions\": f1_eval,\n",
    "                \"reaction_labels\": f1_eval,\n",
    "                \"ranked_reaction_labels\": rank_precision_at_k,\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            fields_to_metric_fn = {\n",
    "                \"serious\": \"exact\",\n",
    "                \"patientsex\": \"exact\",\n",
    "                \"drugs\": f1_eval,\n",
    "                \"reactions\": f1_eval,\n",
    "            }\n",
    "\n",
    "        return fields_to_metric_fn\n",
    "\n",
    "    def getItem(self, idx: int, val: bool=False, include_label: bool=False):\n",
    "        # fetch entry\n",
    "        entry = self.test_dataset[idx] if not val else self.train_dataset[idx]\n",
    "\n",
    "        # create data record\n",
    "        dr = pz.DataRecord(self.schema, source_id=entry['pmid'])\n",
    "        dr.pmid = entry['pmid']\n",
    "        dr.title = entry['title']\n",
    "        dr.abstract = entry['abstract']\n",
    "        dr.fulltext = entry['fulltext']\n",
    "\n",
    "        # if requested, also return the label information\n",
    "        if include_label:\n",
    "            # augment data record with label info\n",
    "            labels_dict = self.label_fields_to_values[entry['pmid']]\n",
    "\n",
    "            for field, value in labels_dict.items():\n",
    "                setattr(dr, field, value)\n",
    "\n",
    "        return dr\n",
    "\n",
    "# create and register validation data source\n",
    "datasource = BiodexValidationSource(\n",
    "    datasetId=f\"biodex-reactions\",\n",
    "    num_samples=50,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    ")\n",
    "DataDirectory().registerUserSource(\n",
    "    src=datasource,\n",
    "    dataset_id=f\"biodex-reactions\",\n",
    ")\n",
    "\n",
    "# load index\n",
    "index_path = \".ragatouille/colbert/indexes/reaction-terms\"\n",
    "index = RAGPretrainedModel.from_index(index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047ec29-7ad2-4958-aa30-2663ae6a06a6",
   "metadata": {},
   "source": [
    "## Define Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc71bd3a-0e92-45fc-bf20-da77cc95e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_str = \"\"\"\n",
    "operators:\n",
    "- MixtureOfAgentsConvert:\n",
    "    proposer_models: [gpt-4o-mini, llama3, gpt-4o]\n",
    "    aggregator_model: llama3\n",
    "    temperatures: [0.4, 0.4, 0.4]\n",
    "- Retrieve:\n",
    "    k: 5\n",
    "- TokenReducedConvertBonded:\n",
    "    model: gpt-4o-mini\n",
    "    token_budget: 0.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a40ff-c380-45fb-84aa-b629ab88becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from palimpzest.constants import Model\n",
    "from palimpzest.operators import *\n",
    "from palimpzest.optimizer import PhysicalPlan\n",
    "import yaml\n",
    "\n",
    "def yaml_to_biodex_reactions_physical_plan(yaml_str):\n",
    "    plan_dict = yaml.safe_load(yaml_str)\n",
    "    operators = plan_dict['operators']\n",
    "\n",
    "    # maps from op_idx to the input and output schema(s)\n",
    "    op_idx_to_input_schema = {\n",
    "        0: BiodexEntry,\n",
    "        1: BiodexReactions,\n",
    "        2: BiodexReactionLabels,\n",
    "    }\n",
    "    op_idx_to_output_schema = {\n",
    "        0: BiodexReactions,\n",
    "        1: BiodexReactionLabels,\n",
    "        2: BiodexRankedReactions,\n",
    "    }\n",
    "    model_name_to_model = {\n",
    "        \"gpt-4o\": Model.GPT_4o,\n",
    "        \"gpt-4o-mini\": Model.GPT_4o_MINI,\n",
    "        \"llama3\": Model.LLAMA3,\n",
    "        \"mixtral\": Model.MIXTRAL,\n",
    "    }\n",
    "\n",
    "    # create scan operator\n",
    "    scanOp = MarshalAndScanDataOp(outputSchema=BiodexEntry, dataset_id=\"biodex-reactions\")\n",
    "\n",
    "    ops = [scanOp]\n",
    "    for op_idx, operator in enumerate(operators):\n",
    "        input_schema = op_idx_to_input_schema[op_idx]\n",
    "        output_schema = op_idx_to_output_schema[op_idx]\n",
    "\n",
    "        op = None\n",
    "        op_name = list(operator.keys())[0]\n",
    "        if \"Retrieve\" == op_name:\n",
    "            op_kwargs = operator[\"Retrieve\"]\n",
    "            op = RetrieveOp(\n",
    "                inputSchema=input_schema,\n",
    "                outputSchema=output_schema,\n",
    "                search_attr=\"reactions\",\n",
    "                output_attr=\"reaction_labels\",\n",
    "                index=index,\n",
    "                verbose=True,\n",
    "                **op_kwargs,\n",
    "            )\n",
    "        elif \"MixtureOfAgentsConvert\" == op_name:\n",
    "            op_kwargs = operator[\"MixtureOfAgentsConvert\"]\n",
    "            temperatures = op_kwargs['temperatures']            \n",
    "            proposer_models = [model_name_to_model[model_name] for model_name in op_kwargs[\"proposer_models\"]]\n",
    "            aggregator_model = model_name_to_model[op_kwargs['aggregator_model']]\n",
    "            depends_on = op_kwargs[\"depends_on\"] if \"depends_on\" in op_kwargs else None\n",
    "            op = MixtureOfAgentsConvert(\n",
    "                inputSchema=input_schema,\n",
    "                outputSchema=output_schema,\n",
    "                proposer_models=proposer_models,\n",
    "                aggregator_model=aggregator_model,\n",
    "                temperatures=temperatures,\n",
    "                verbose=True,\n",
    "                depends_on=depends_on,\n",
    "            )\n",
    "        elif \"LLMConvertBonded\" == op_name:\n",
    "            op_kwargs = operator[\"LLMConvertBonded\"]\n",
    "            model = model_name_to_model[op_kwargs[\"model\"]]\n",
    "            depends_on = op_kwargs[\"depends_on\"] if \"depends_on\" in op_kwargs else None\n",
    "            op = LLMConvertBonded(\n",
    "                inputSchema=input_schema,\n",
    "                outputSchema=output_schema,\n",
    "                verbose=True,\n",
    "                model=model,\n",
    "                depends_on=depends_on,\n",
    "            )\n",
    "        elif \"TokenReducedConvertBonded\" == op_name:\n",
    "            op_kwargs = operator[\"TokenReducedConvertBonded\"]\n",
    "            model = model_name_to_model[op_kwargs[\"model\"]]\n",
    "            token_budget = op_kwargs[\"token_budget\"]\n",
    "            depends_on = op_kwargs[\"depends_on\"] if \"depends_on\" in op_kwargs else None\n",
    "            op = TokenReducedConvertBonded(\n",
    "                inputSchema=input_schema,\n",
    "                outputSchema=output_schema,\n",
    "                verbose=True,\n",
    "                model=model,\n",
    "                token_budget=token_budget,\n",
    "                depends_on=depends_on,\n",
    "            )\n",
    "\n",
    "        ops.append(op)\n",
    "\n",
    "    # construct final plan\n",
    "    plan = PhysicalPlan(operators=ops)\n",
    "\n",
    "    return plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef6913-f590-4469-aca2-3283e90fff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = yaml_to_biodex_reactions_physical_plan(yaml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0205430-6574-4a9a-ad3c-3092be1bd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74e4e8-f1d5-4243-bebe-61b075734038",
   "metadata": {},
   "source": [
    "## Single-Threaded Execution (works for all policies, but slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5ea86-093b-4d85-8e56-ce5f1d3a08ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from palimpzest.corelib import SourceRecord\n",
    "from palimpzest.dataclasses import OperatorStats, PlanStats\n",
    "from palimpzest.datamanager import DataDirectory\n",
    "from palimpzest.operators import DataSourcePhysicalOp\n",
    "\n",
    "plan_start_time = time.time()\n",
    "\n",
    "# initialize plan stats and operator stats\n",
    "plan_stats = PlanStats(plan_id=plan.plan_id, plan_str=str(plan))\n",
    "for op in plan.operators:\n",
    "    op_id = op.get_op_id()\n",
    "    op_name = op.op_name()\n",
    "    op_details = {k: str(v) for k, v in op.get_op_params().items()}\n",
    "    plan_stats.operator_stats[op_id] = OperatorStats(op_id=op_id, op_name=op_name, op_details=op_details)\n",
    "\n",
    "# get handle to DataSource and pre-compute its size\n",
    "source_operator = plan.operators[0]\n",
    "datasource = DataDirectory().getRegisteredDataset(source_operator.dataset_id)\n",
    "datasource_len = len(datasource)\n",
    "\n",
    "# execute the plan one operator at a time\n",
    "candidates, output_records = [], []\n",
    "for current_scan_idx in range(datasource_len):\n",
    "    for op_idx, operator in enumerate(plan.operators):\n",
    "        op_id = operator.get_op_id()\n",
    "        prev_op_id = (\n",
    "            plan.operators[op_idx - 1].get_op_id() if op_idx > 1 else None\n",
    "        )\n",
    "        next_op_id = (\n",
    "            plan.operators[op_idx + 1].get_op_id()\n",
    "            if op_idx + 1 < len(plan.operators)\n",
    "            else None\n",
    "        )\n",
    "    \n",
    "        # initialize output records and record_op_stats for this operator\n",
    "        records, record_op_stats = [], []\n",
    "\n",
    "        # invoke datasource operator(s) until we run out of source records or hit the num_samples limit\n",
    "        if isinstance(operator, DataSourcePhysicalOp):\n",
    "            # construct input DataRecord for DataSourcePhysicalOp\n",
    "            # NOTE: this DataRecord will be discarded and replaced by the scan_operator;\n",
    "            #       it is simply a vessel to inform the scan_operator which record to fetch\n",
    "            candidate = DataRecord(schema=SourceRecord, source_id=current_scan_idx)\n",
    "            candidate.idx = current_scan_idx\n",
    "            candidate.get_item_fn = datasource.getItem\n",
    "\n",
    "            # run DataSourcePhysicalOp on record\n",
    "            record_set = operator(candidate)\n",
    "            records.extend(record_set.data_records)\n",
    "            record_op_stats.extend(record_set.record_op_stats)\n",
    "\n",
    "        # otherwise, process the records in the processing queue for this operator one at a time\n",
    "        else:\n",
    "            for input_record in candidates:\n",
    "                record_set = operator(input_record)\n",
    "                records.extend(record_set.data_records)\n",
    "                record_op_stats.extend(record_set.record_op_stats)\n",
    "\n",
    "        # update plan stats\n",
    "        plan_stats.operator_stats[op_id].add_record_op_stats(\n",
    "            record_op_stats,\n",
    "            source_op_id=prev_op_id,\n",
    "            plan_id=plan.plan_id,\n",
    "        )\n",
    "\n",
    "        # update candidates\n",
    "        candidates = []\n",
    "        for record in records:\n",
    "            if isinstance(operator, FilterOp):\n",
    "                if not record._passed_operator:\n",
    "                    continue\n",
    "            if next_op_id is not None:\n",
    "                candidates.append(record)\n",
    "            else:\n",
    "                output_records.append(record)\n",
    "\n",
    "        # if we've filtered out all records, terminate early\n",
    "        if next_op_id is not None:\n",
    "            if candidates == []:\n",
    "                break\n",
    "\n",
    "# finalize plan stats\n",
    "total_plan_time = time.time() - plan_start_time\n",
    "plan_stats.finalize(total_plan_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056f126-b340-45a2-a2f7-6040a0b24b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f3f4f90-0600-4d57-b599-c9954e799100",
   "metadata": {},
   "source": [
    "## Parallel Execution (fast; but does not work for time policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8a5f2-4e72-48c0-a4c6-6f0f49cda1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from palimpzest.constants import PARALLEL_EXECUTION_SLEEP_INTERVAL_SECS\n",
    "from palimpzest.corelib import SourceRecord\n",
    "from palimpzest.dataclasses import OperatorStats, PlanStats\n",
    "from palimpzest.datamanager import DataDirectory\n",
    "from palimpzest.operators import DataSourcePhysicalOp\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "def execute_op_wrapper(operator, op_input):\n",
    "    \"\"\"\n",
    "    Wrapper function around operator execution which also and returns the operator.\n",
    "    This is useful in the parallel setting(s) where operators are executed by a worker pool,\n",
    "    and it is convenient to return the op_id along with the computation result.\n",
    "    \"\"\"\n",
    "    record_set = operator(op_input)\n",
    "\n",
    "    return record_set, operator\n",
    "\n",
    "plan_start_time = time.time()\n",
    "\n",
    "# initialize plan stats and operator stats\n",
    "plan_stats = PlanStats(plan_id=plan.plan_id, plan_str=str(plan))\n",
    "for op in plan.operators:\n",
    "    op_id = op.get_op_id()\n",
    "    op_name = op.op_name()\n",
    "    op_details = {k: str(v) for k, v in op.get_op_params().items()}\n",
    "    plan_stats.operator_stats[op_id] = OperatorStats(op_id=op_id, op_name=op_name, op_details=op_details)\n",
    "\n",
    "# initialize list of output records and intermediate variables\n",
    "output_records = []\n",
    "source_records_scanned = 0\n",
    "\n",
    "# initialize data structures to help w/processing DAG\n",
    "processing_queue = []\n",
    "op_id_to_futures_in_flight = {op.get_op_id(): 0 for op in plan.operators}\n",
    "op_id_to_operator = {op.get_op_id(): op for op in plan.operators}\n",
    "op_id_to_prev_operator = {\n",
    "    op.get_op_id(): plan.operators[idx - 1] if idx > 0 else None\n",
    "    for idx, op in enumerate(plan.operators)\n",
    "}\n",
    "op_id_to_next_operator = {\n",
    "    op.get_op_id(): plan.operators[idx + 1] if idx + 1 < len(plan.operators) else None\n",
    "    for idx, op in enumerate(plan.operators)\n",
    "}\n",
    "op_id_to_op_idx = {op.get_op_id(): idx for idx, op in enumerate(plan.operators)}\n",
    "\n",
    "# get handle to DataSource and pre-compute its size\n",
    "source_operator = plan.operators[0]\n",
    "source_op_id = source_operator.get_op_id()\n",
    "datasource = DataDirectory().getRegisteredDataset(source_operator.dataset_id)\n",
    "datasource_len = len(datasource)\n",
    "\n",
    "# create thread pool w/max workers\n",
    "futures = []\n",
    "current_scan_idx = 0\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    # create initial (set of) future(s) to read first source record;\n",
    "    # construct input DataRecord for DataSourcePhysicalOp\n",
    "    # NOTE: this DataRecord will be discarded and replaced by the scan_operator;\n",
    "    #       it is simply a vessel to inform the scan_operator which record to fetch\n",
    "    candidate = DataRecord(schema=SourceRecord, source_id=current_scan_idx)\n",
    "    candidate.idx = current_scan_idx\n",
    "    candidate.get_item_fn = datasource.getItem\n",
    "    futures.append(executor.submit(execute_op_wrapper, source_operator, candidate))\n",
    "    op_id_to_futures_in_flight[source_op_id] += 1\n",
    "    current_scan_idx += 1\n",
    "\n",
    "    # iterate until we have processed all operators on all records or come to an early stopping condition\n",
    "    while len(futures) > 0:\n",
    "        # get the set of futures that have (and have not) finished in the last PARALLEL_EXECUTION_SLEEP_INTERVAL_SECS\n",
    "        done_futures, not_done_futures = wait(futures, timeout=PARALLEL_EXECUTION_SLEEP_INTERVAL_SECS)\n",
    "\n",
    "        # cast not_done_futures from a set to a list so we can append to it\n",
    "        not_done_futures = list(not_done_futures)\n",
    "\n",
    "        # process finished futures, creating new ones as needed\n",
    "        new_futures = []\n",
    "        for future in done_futures:\n",
    "            # get the result\n",
    "            record_set, operator = future.result()\n",
    "            op_id = operator.get_op_id()\n",
    "\n",
    "            # decrement future from mapping of futures in-flight\n",
    "            op_id_to_futures_in_flight[op_id] -= 1\n",
    "\n",
    "            # update plan stats\n",
    "            prev_operator = op_id_to_prev_operator[op_id]\n",
    "            plan_stats.operator_stats[op_id].add_record_op_stats(\n",
    "                record_set.record_op_stats,\n",
    "                source_op_id=prev_operator.get_op_id() if prev_operator is not None else None,\n",
    "                plan_id=plan.plan_id,\n",
    "            )\n",
    "\n",
    "            # process each record output by the future's operator\n",
    "            for record in record_set:\n",
    "                # skip records which are filtered out\n",
    "                if not getattr(record, \"_passed_operator\", True):\n",
    "                    continue\n",
    "\n",
    "                # add records to processing queue if there is a next_operator; otherwise add to output_records\n",
    "                next_operator = op_id_to_next_operator[op_id]\n",
    "                if next_operator is not None:\n",
    "                    processing_queue.append((next_operator, record))\n",
    "                else:\n",
    "                    output_records.append(record)\n",
    "\n",
    "            # if this operator was a source scan, update the number of source records scanned\n",
    "            if op_id == source_op_id:\n",
    "                source_records_scanned += len(record_set)\n",
    "\n",
    "                # scan next record if we can still draw records from source\n",
    "                if current_scan_idx < datasource_len:\n",
    "                    # construct input DataRecord for DataSourcePhysicalOp\n",
    "                    # NOTE: this DataRecord will be discarded and replaced by the scan_operator;\n",
    "                    #       it is simply a vessel to inform the scan_operator which record to fetch\n",
    "                    candidate = DataRecord(schema=SourceRecord, source_id=current_scan_idx)\n",
    "                    candidate.idx = current_scan_idx\n",
    "                    candidate.get_item_fn = datasource.getItem\n",
    "                    new_futures.append(executor.submit(execute_op_wrapper, source_operator, candidate))\n",
    "                    op_id_to_futures_in_flight[source_op_id] += 1\n",
    "                    current_scan_idx += 1\n",
    "\n",
    "        # process all records in the processing queue which are ready to be executed\n",
    "        for operator, candidate in processing_queue:\n",
    "            # if the candidate is not an input to an aggregate, execute it right away\n",
    "            future = executor.submit(execute_op_wrapper, operator, candidate)\n",
    "            new_futures.append(future)\n",
    "            op_id_to_futures_in_flight[operator.get_op_id()] += 1\n",
    "\n",
    "        # clear the processing queue\n",
    "        processing_queue = []\n",
    "\n",
    "        # update list of futures\n",
    "        not_done_futures.extend(new_futures)\n",
    "        futures = not_done_futures\n",
    "\n",
    "# finalize plan stats\n",
    "total_plan_time = time.time() - plan_start_time\n",
    "plan_stats.finalize(total_plan_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605ec4a-ede4-4f28-aa72-84262b6e7577",
   "metadata": {},
   "source": [
    "## Compute Plan Cost, Time, and Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232da65-4a57-4662-be2e-4985fd4690df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quality(record_set, expected_record_set, field_to_metric_fn):\n",
    "    \"\"\"\n",
    "    Compute the quality for the given `record_set` by comparing it to the `expected_record_set`.\n",
    "    \n",
    "    Update the record_set by assigning the quality to each entry in its record_op_stats and\n",
    "    returning the updated record_set.\n",
    "    \"\"\"\n",
    "    # if this operation is a failed convert\n",
    "    if len(record_set) == 0:\n",
    "        record_set.record_op_stats[0].quality = 0.0\n",
    "\n",
    "    # if this is a successful convert operation\n",
    "    # NOTE: the following computation assumes we do not project out computed values\n",
    "    #       (and that the validation examples provide all computed fields); even if\n",
    "    #       a user program does add projection, we can ignore the projection on the\n",
    "    #       validation dataset and use the champion model (as opposed to the validation\n",
    "    #       output) for scoring fields which have their values projected out\n",
    "\n",
    "    # GREEDY ALGORITHM\n",
    "    # for each record in the expected output, we look for the computed record which maximizes the quality metric;\n",
    "    # once we've identified that computed record we remove it from consideration for the next expected output\n",
    "    for expected_record in expected_record_set:\n",
    "        best_quality, best_record_op_stats = 0.0, None\n",
    "        for record_op_stats in record_set.record_op_stats:\n",
    "            # if we already assigned this record a quality, skip it\n",
    "            if record_op_stats.quality is not None:\n",
    "                continue\n",
    "\n",
    "            # compute number of matches between this record's computed fields and this expected record's outputs\n",
    "            total_quality = 0\n",
    "            for field in record_op_stats.generated_fields:\n",
    "                computed_value = record_op_stats.record_state.get(field, None)\n",
    "                expected_value = getattr(expected_record, field)\n",
    "\n",
    "                # get the metric function for this field\n",
    "                metric_fn = (\n",
    "                    field_to_metric_fn[field]\n",
    "                    if field_to_metric_fn is not None and field in field_to_metric_fn\n",
    "                    else \"exact\"\n",
    "                )\n",
    "\n",
    "                # compute exact match\n",
    "                if metric_fn == \"exact\":\n",
    "                    total_quality += int(computed_value == expected_value)\n",
    "\n",
    "                # compute UDF metric\n",
    "                elif callable(metric_fn):\n",
    "                    total_quality += metric_fn(computed_value, expected_value)\n",
    "\n",
    "                # otherwise, throw an exception\n",
    "                else:\n",
    "                    raise Exception(f\"Unrecognized metric_fn: {metric_fn}\")\n",
    "\n",
    "            # compute recall and update best seen so far\n",
    "            quality = total_quality / len(record_op_stats.generated_fields)\n",
    "            if quality > best_quality:\n",
    "                best_quality = quality\n",
    "                best_record_op_stats = record_op_stats\n",
    "\n",
    "        # set best_quality as quality for the best_record_op_stats\n",
    "        if best_record_op_stats is not None:\n",
    "            best_record_op_stats.quality = best_quality\n",
    "\n",
    "    # for any records which did not receive a quality, set it to 0.0 as these are unexpected extras\n",
    "    for record_op_stats in record_set.record_op_stats:\n",
    "        if record_op_stats.quality is None:\n",
    "            record_op_stats.quality = 0.0\n",
    "    \n",
    "    return record_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f505bed-1503-4502-8c18-3d618b2bc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute plan cost, runtime, and quality\n",
    "plan_cost = plan_stats.total_plan_cost\n",
    "plan_time = plan_stats.total_plan_time\n",
    "\n",
    "expected_outputs = {}\n",
    "for idx in range(datasource_len):\n",
    "    data_record = datasource.getItem(idx, include_label=True)\n",
    "    expected_outputs[data_record._source_id] = data_record\n",
    "\n",
    "qualities = []\n",
    "for record in output_records:\n",
    "    expected_record = expected_outputs[record._source_id]\n",
    "    fields_to_metric_fn = datasource.getFieldToMetricFn()\n",
    "    metric_fn = fields_to_metric_fn[\"ranked_reaction_labels\"]\n",
    "    computed_value = getattr(record, \"ranked_reaction_labels\")\n",
    "    expected_value = getattr(expected_record, \"ranked_reaction_labels\")\n",
    "    quality = metric_fn(computed_value, expected_value)\n",
    "    qualities.append(quality)\n",
    "\n",
    "plan_quality = sum(qualities)/len(qualities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04ec2e-99b3-4e4a-a5e4-2f07414f04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Plan(cost={plan_cost:.5f}, time={plan_time:.2f}, quality={plan_quality:.5f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dee24e-353c-49d0-81ea-c06699f128ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5ee4085-3399-4c05-b9f5-c0a8f4260a0a",
   "metadata": {},
   "source": [
    "## If you want to examine train / test dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2ba85-c3fd-4527-a623-3dab646d4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"BioDEX/BioDEX-ICSR\")\n",
    "train_dataset = [dataset['train'][idx] for idx in range(100)]\n",
    "test_dataset = [dataset['train'][idx] for idx in range(100, 150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f271f9-10e7-42e6-854d-c26a0fb8dc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
