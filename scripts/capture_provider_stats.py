#!/usr/bin/env python3
"""
Script to directly invoke LLM providers and capture token/cost statistics.

This script:
1. Loads messages from JSON files generated by generate_test_messages.py
2. Sends requests directly to each provider's API (not through litellm)
3. Saves the token/cost related stats returned by the provider
4. Waits 10 seconds
5. Sends the request again and saves the second set of stats

This allows us to establish baseline expectations for what the providers return,
which can then be used to validate the palimpzest generator's stats tracking.

Supported providers:
- Anthropic: claude-sonnet-4-5-20250929 (text, image, text+image)
- Google/Vertex AI: gemini-2.5-flash (all seven modality combinations)
- OpenAI: gpt-4o-2024-08-06 (text, image, text+image)
- OpenAI: gpt-4o-audio-preview (text+audio, audio)

Output files are saved to: tests/pytest/scripts/provider_stats/
"""

import argparse
import base64
import json
import os
import sys
import time
import uuid
from typing import Any


def detect_image_media_type(base64_data: str) -> str:
    """
    Detect the actual image format from base64 data by examining the magic bytes.

    Args:
        base64_data: Base64-encoded image data.

    Returns:
        The detected media type (e.g., 'image/png', 'image/jpeg').
        Defaults to 'image/jpeg' if format cannot be determined.
    """
    try:
        # Decode first few bytes to check magic numbers
        header = base64.b64decode(base64_data[:32])

        # PNG: 89 50 4E 47 0D 0A 1A 0A
        if header[:8] == b"\x89PNG\r\n\x1a\n":
            return "image/png"
        # JPEG: FF D8 FF
        if header[:3] == b"\xff\xd8\xff":
            return "image/jpeg"
        # GIF: GIF87a or GIF89a
        if header[:6] in (b"GIF87a", b"GIF89a"):
            return "image/gif"
        # WebP: RIFF....WEBP
        if header[:4] == b"RIFF" and header[8:12] == b"WEBP":
            return "image/webp"
    except Exception:
        pass

    return "image/jpeg"

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))


# =============================================================================
# PROVIDER CONFIGURATIONS
# =============================================================================
PROVIDER_MODALITY_SUPPORT = {
    "anthropic": {
        "model": "claude-sonnet-4-5-20250929",
        "supported_modalities": ["text-only", "image-only", "text-image"],
        # Note: Anthropic does not support audio
    },
    "openai": {
        "model": "gpt-4o-2024-08-06",
        "supported_modalities": ["text-only", "image-only", "text-image"],
    },
    "openai-audio": {
        "model": "gpt-4o-audio-preview",
        "supported_modalities": ["audio-only", "text-audio"],
    },
    "gemini": {
        "model": "gemini-2.5-flash",
        "supported_modalities": [
            "text-only",
            "image-only",
            "audio-only",
            "text-image",
            "text-audio",
            "image-audio",
            "text-image-audio",
        ],
    },
    "vertex_ai": {
        "model": "gemini-2.5-flash",
        "supported_modalities": [
            "text-only",
            "image-only",
            "audio-only",
            "text-image",
            "text-audio",
            "image-audio",
            "text-image-audio",
        ],
    },
}


def load_messages(modality: str, provider: str, messages_dir: str) -> list[dict]:
    """Load messages from JSON file for a given modality/provider combination."""
    filepath = os.path.join(messages_dir, f"{modality}_{provider}.json")
    with open(filepath, "r") as f:
        return json.load(f)


def transform_messages_for_openai(messages: list[dict]) -> list[dict]:
    """
    Transform palimpzest/litellm message format to OpenAI API format.

    OpenAI expects:
    - system messages with string content
    - user messages with string content or array of content parts

    Input messages may have content as string or list of content blocks.
    """
    openai_messages = []

    for msg in messages:
        role = msg.get("role")
        msg_type = msg.get("type")
        content = msg.get("content")

        if role == "system":
            # System content may be string or list of content blocks
            if isinstance(content, list):
                # Extract text from content blocks
                text_parts = [block.get("text", "") for block in content if block.get("type") == "text"]
                openai_messages.append({"role": "system", "content": "".join(text_parts)})
            else:
                openai_messages.append({"role": "system", "content": content})

        elif role == "user":
            if msg_type == "text":
                # Content may be string or list of content blocks
                if isinstance(content, list):
                    # Already content blocks - add them directly
                    content_parts = []
                    for block in content:
                        # Convert to OpenAI format (remove cache_control if present)
                        openai_block = {"type": block.get("type", "text")}
                        if block.get("type") == "text":
                            openai_block["text"] = block.get("text", "")
                        content_parts.append(openai_block)

                    if openai_messages and openai_messages[-1]["role"] == "user":
                        existing_content = openai_messages[-1]["content"]
                        if isinstance(existing_content, str):
                            openai_messages[-1]["content"] = [
                                {"type": "text", "text": existing_content}
                            ] + content_parts
                        else:
                            existing_content.extend(content_parts)
                    else:
                        openai_messages.append({"role": "user", "content": content_parts})
                else:
                    # String content
                    if openai_messages and openai_messages[-1]["role"] == "user":
                        existing_content = openai_messages[-1]["content"]
                        if isinstance(existing_content, str):
                            openai_messages[-1]["content"] = [
                                {"type": "text", "text": existing_content},
                                {"type": "text", "text": content},
                            ]
                        else:
                            existing_content.append({"type": "text", "text": content})
                    else:
                        openai_messages.append({"role": "user", "content": content})

            elif msg_type == "image":
                # Image content
                image_parts = []
                for img in content:
                    if img.get("type") == "image_url":
                        image_parts.append(img)

                if openai_messages and openai_messages[-1]["role"] == "user":
                    existing_content = openai_messages[-1]["content"]
                    if isinstance(existing_content, str):
                        openai_messages[-1]["content"] = [
                            {"type": "text", "text": existing_content}
                        ] + image_parts
                    else:
                        existing_content.extend(image_parts)
                else:
                    openai_messages.append({"role": "user", "content": image_parts})

            elif msg_type == "input_audio":
                # Audio content
                audio_parts = []
                for audio in content:
                    if audio.get("type") == "input_audio":
                        audio_parts.append(audio)

                if openai_messages and openai_messages[-1]["role"] == "user":
                    existing_content = openai_messages[-1]["content"]
                    if isinstance(existing_content, str):
                        openai_messages[-1]["content"] = [
                            {"type": "text", "text": existing_content}
                        ] + audio_parts
                    else:
                        existing_content.extend(audio_parts)
                else:
                    openai_messages.append({"role": "user", "content": audio_parts})

    return openai_messages


def transform_messages_for_anthropic(messages: list[dict]) -> tuple[str | None, list[dict]]:
    """
    Transform palimpzest/litellm message format to Anthropic API format.

    Input messages may already have cache_control markers from PromptCacheManager.
    This function preserves those markers while converting to Anthropic's native format.

    Anthropic expects:
    - system as a separate parameter (not in messages)
    - user/assistant messages with content as array of content blocks
    - cache_control markers for caching (preserved from input)
    """
    system_prompt = None
    anthropic_messages = []

    for msg in messages:
        role = msg.get("role")
        msg_type = msg.get("type")
        content = msg.get("content")

        if role == "system":
            # Anthropic uses system as a separate parameter
            # Content may already be a list of content blocks with cache_control
            if isinstance(content, list):
                # Already in content block format (from PromptCacheManager)
                system_prompt = content
            else:
                # String content - wrap in content block with cache_control
                system_prompt = [
                    {
                        "type": "text",
                        "text": content,
                        "cache_control": {"type": "ephemeral"},
                    }
                ]

        elif role == "user":
            if msg_type == "text":
                # Content may be string or list of content blocks
                if isinstance(content, list):
                    # Already content blocks (may have cache_control) - preserve them
                    for block in content:
                        if anthropic_messages and anthropic_messages[-1]["role"] == "user":
                            anthropic_messages[-1]["content"].append(block)
                        else:
                            anthropic_messages.append({"role": "user", "content": [block]})
                else:
                    # String content
                    content_block = {"type": "text", "text": content}
                    if anthropic_messages and anthropic_messages[-1]["role"] == "user":
                        anthropic_messages[-1]["content"].append(content_block)
                    else:
                        anthropic_messages.append({"role": "user", "content": [content_block]})

            elif msg_type == "image":
                # Image content - Anthropic uses base64 format
                for img in content:
                    if img.get("type") == "image_url":
                        url = img["image_url"]["url"]
                        if url.startswith("data:"):
                            # Extract base64 data
                            _, data = url.split(";base64,")
                            # Detect actual media type from image data (in case URL has wrong type)
                            media_type = detect_image_media_type(data)
                            image_block = {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": media_type,
                                    "data": data,
                                },
                            }
                            # Preserve cache_control if present on the original block
                            if "cache_control" in img:
                                image_block["cache_control"] = img["cache_control"]
                            if anthropic_messages and anthropic_messages[-1]["role"] == "user":
                                anthropic_messages[-1]["content"].append(image_block)
                            else:
                                anthropic_messages.append({"role": "user", "content": [image_block]})

    return system_prompt, anthropic_messages


def transform_messages_for_gemini(messages: list[dict]) -> tuple[str | None, list[dict]]:
    """
    Transform palimpzest/litellm message format to Gemini API format.

    Gemini expects:
    - role: "user" or "model"
    - parts: list of content parts

    Input messages may have content as string or list of content blocks.
    """
    gemini_contents = []
    system_instruction = None

    for msg in messages:
        role = msg.get("role")
        msg_type = msg.get("type")
        content = msg.get("content")

        if role == "system":
            # Gemini uses system_instruction
            # Content may be string or list of content blocks
            if isinstance(content, list):
                # Extract text from content blocks
                text_parts = [block.get("text", "") for block in content if block.get("type") == "text"]
                system_instruction = "".join(text_parts)
            else:
                system_instruction = content

        elif role == "user":
            parts = []

            if msg_type == "text":
                # Content may be string or list of content blocks
                if isinstance(content, list):
                    for block in content:
                        if block.get("type") == "text":
                            parts.append({"text": block.get("text", "")})
                else:
                    parts.append({"text": content})

            elif msg_type == "image":
                for img in content:
                    if img.get("type") == "image_url":
                        url = img["image_url"]["url"]
                        if url.startswith("data:"):
                            _, data = url.split(";base64,")
                            # Detect actual media type from image data
                            media_type = detect_image_media_type(data)
                            parts.append({
                                "inline_data": {
                                    "mime_type": media_type,
                                    "data": data,
                                }
                            })

            elif msg_type == "input_audio":
                for audio in content:
                    if audio.get("type") == "input_audio":
                        audio_data = audio["input_audio"]
                        parts.append({
                            "inline_data": {
                                "mime_type": f"audio/{audio_data.get('format', 'wav')}",
                                "data": audio_data["data"],
                            }
                        })

            if parts:
                if gemini_contents and gemini_contents[-1]["role"] == "user":
                    gemini_contents[-1]["parts"].extend(parts)
                else:
                    gemini_contents.append({"role": "user", "parts": parts})

    return system_instruction, gemini_contents


def call_openai_api(messages: list[dict], model: str, cache_key: str | None = None) -> dict[str, Any]:
    """
    Call OpenAI API directly and return usage statistics.

    Args:
        messages: List of message dicts
        model: Model name
        cache_key: Optional prompt_cache_key for sticky routing to same cache shard

    Returns dict with:
    - completion_tokens
    - prompt_tokens
    - prompt_tokens_details (cached_tokens, text_tokens, image_tokens, audio_tokens)
    - total_tokens
    """
    import openai

    client = openai.OpenAI()

    openai_messages = transform_messages_for_openai(messages)

    kwargs = {"model": model, "messages": openai_messages, "temperature": 0.0}

    # Check if this is an audio model
    if "audio" in model:
        kwargs["modalities"] = ["text"]

    # Add prompt_cache_key for caching (ensures requests route to same cache shard)
    if cache_key:
        kwargs["extra_body"] = {"prompt_cache_key": cache_key}

    response = client.chat.completions.create(**kwargs)

    # Extract complete usage stats
    usage_dict = {}
    if response.usage:
        usage_dict = response.usage.model_dump()

    # Get response text safely
    try:
        response_text = response.choices[0].message.content[:200] if response.choices and response.choices[0].message.content else None
    except Exception:
        response_text = None

    # Serialize the full response
    try:
        raw_response = response.model_dump()
    except Exception:
        raw_response = str(response)

    return {
        "provider": "openai",
        "model": model,
        "usage": usage_dict,
        "response_content": response_text,
        "raw_response": raw_response,
    }


def call_anthropic_api(messages: list[dict], model: str) -> dict[str, Any]:
    """
    Call Anthropic API directly and return usage statistics.

    Returns dict with:
    - input_tokens
    - output_tokens
    - cache_creation_input_tokens
    - cache_read_input_tokens
    """
    import anthropic

    client = anthropic.Anthropic()

    system_prompt, anthropic_messages = transform_messages_for_anthropic(messages)

    response = client.messages.create(
        model=model,
        max_tokens=1024,
        system=system_prompt,
        messages=anthropic_messages,
    )

    # Extract complete usage stats
    usage_dict = {}
    if response.usage:
        usage_dict = response.usage.model_dump()

    # Get response text safely
    try:
        response_text = response.content[0].text[:200] if response.content and response.content[0].text else None
    except Exception:
        response_text = None

    # Serialize the full response
    try:
        raw_response = response.model_dump()
    except Exception:
        raw_response = str(response)

    return {
        "provider": "anthropic",
        "model": model,
        "usage": usage_dict,
        "response_content": response_text,
        "raw_response": raw_response,
    }


def call_gemini_api(messages: list[dict], model: str, use_vertex: bool = False) -> dict[str, Any]:
    """
    Call Gemini API directly and return usage statistics.

    Args:
        messages: List of message dicts
        model: Model name
        use_vertex: If True, use Vertex AI; otherwise use Google AI Studio

    Returns dict with usage statistics.
    """
    from google import genai
    from google.genai import types

    system_instruction, gemini_contents = transform_messages_for_gemini(messages)

    # Create client for Google AI Studio or Vertex AI
    if use_vertex:
        # Vertex AI requires project and location
        import os
        client = genai.Client(
            vertexai=True,
            project=os.environ.get("GOOGLE_CLOUD_PROJECT", os.environ.get("VERTEXAI_PROJECT")),
            location=os.environ.get("GOOGLE_CLOUD_LOCATION", os.environ.get("VERTEXAI_LOCATION", "us-central1")),
        )
    else:
        # Google AI Studio uses API key from environment
        client = genai.Client()

    # Build the config
    config = types.GenerateContentConfig(
        temperature=0.0,
        system_instruction=system_instruction if system_instruction else None,
    )

    response = client.models.generate_content(
        model=model,
        contents=gemini_contents,
        config=config,
    )

    # Extract complete usage stats from usage_metadata
    usage_metadata = response.usage_metadata
    usage_dict = {}
    if usage_metadata:
        # Try model_dump() first (Pydantic models), then to_dict(), then manual extraction
        try:
            usage_dict = usage_metadata.model_dump()
        except AttributeError:
            try:
                usage_dict = usage_metadata.to_dict()
            except AttributeError:
                # Manual extraction of known Gemini usage fields
                usage_dict = {
                    "prompt_token_count": getattr(usage_metadata, "prompt_token_count", None),
                    "candidates_token_count": getattr(usage_metadata, "candidates_token_count", None),
                    "total_token_count": getattr(usage_metadata, "total_token_count", None),
                    "cached_content_token_count": getattr(usage_metadata, "cached_content_token_count", None),
                }

    # Get response text safely
    try:
        response_text = response.text[:200] if response.text else None
    except Exception:
        response_text = None

    # Serialize the full response
    try:
        # Try model_dump() first (Pydantic models)
        raw_response = response.model_dump()
    except AttributeError:
        try:
            raw_response = response.to_dict()
        except AttributeError:
            # Manual serialization
            try:
                raw_response = {
                    "text": response.text if hasattr(response, "text") else None,
                    "candidates": [
                        {
                            "content": {
                                "parts": [{"text": getattr(part, "text", str(part))} for part in c.content.parts] if c.content and c.content.parts else [],
                                "role": c.content.role if c.content else None,
                            },
                            "finish_reason": str(c.finish_reason) if hasattr(c, "finish_reason") else None,
                        }
                        for c in (response.candidates or [])
                    ],
                    "usage_metadata": usage_dict,
                    "model_version": getattr(response, "model_version", None),
                }
            except Exception as e:
                raw_response = {"error": str(e), "response_str": str(response)}

    return {
        "provider": "vertex_ai" if use_vertex else "gemini",
        "model": model,
        "usage": usage_dict,
        "response_content": response_text,
        "raw_response": raw_response,
    }


def capture_stats_for_provider(
    provider: str,
    modality: str,
    messages: list[dict],
    model: str,
) -> dict[str, Any]:
    """
    Capture stats for a provider by making two requests with a 10-second gap.

    Returns dict with:
    - first_request: stats from first request
    - second_request: stats from second request (should show cache hits)
    """
    # Generate a unique cache key for OpenAI (ensures both requests hit the same cache shard)
    openai_cache_key = f"pz-test-{uuid.uuid4().hex[:12]}" if provider in ("openai", "openai-audio") else None

    print(f"    First request...")
    if provider == "openai" or provider == "openai-audio":
        first_stats = call_openai_api(messages, model, cache_key=openai_cache_key)
    elif provider == "anthropic":
        first_stats = call_anthropic_api(messages, model)
    elif provider == "gemini":
        first_stats = call_gemini_api(messages, model, use_vertex=False)
    elif provider == "vertex_ai":
        first_stats = call_gemini_api(messages, model, use_vertex=True)
    else:
        raise ValueError(f"Unknown provider: {provider}")

    print(f"      Usage: {first_stats['usage']}")

    print(f"    Waiting 20 seconds for cache to be available...")
    time.sleep(20)

    print(f"    Second request (should show cache hits)...")
    if provider == "openai" or provider == "openai-audio":
        second_stats = call_openai_api(messages, model, cache_key=openai_cache_key)
    elif provider == "anthropic":
        second_stats = call_anthropic_api(messages, model)
    elif provider == "gemini":
        second_stats = call_gemini_api(messages, model, use_vertex=False)
    elif provider == "vertex_ai":
        second_stats = call_gemini_api(messages, model, use_vertex=True)

    print(f"      Usage: {second_stats['usage']}")

    return {
        "provider": provider,
        "model": model,
        "modality": modality,
        "first_request": first_stats,
        "second_request": second_stats,
    }


def save_stats(stats: dict[str, Any], output_dir: str, provider: str, modality: str) -> str:
    """Save stats to a JSON file."""
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{provider}_{modality}.json")

    with open(output_path, "w") as f:
        json.dump(stats, f, indent=2)

    return output_path


def main():
    """Capture provider stats for supported provider/modality combinations."""
    parser = argparse.ArgumentParser(
        description="Capture token/cost statistics from LLM providers.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Available providers: {', '.join(PROVIDER_MODALITY_SUPPORT.keys())}
Available modalities: text-only, image-only, audio-only, text-image, text-audio, image-audio, text-image-audio

Examples:
  python capture_provider_stats.py                              # Run all providers/modalities
  python capture_provider_stats.py -p openai                    # Run all modalities for OpenAI
  python capture_provider_stats.py -p openai -m text-only       # Run only text-only for OpenAI
  python capture_provider_stats.py -p anthropic -m text-image   # Run only text-image for Anthropic
        """
    )
    parser.add_argument(
        "-p", "--provider",
        nargs="+",
        choices=list(PROVIDER_MODALITY_SUPPORT.keys()),
        help="Provider(s) to run. If not specified, runs all providers.",
    )
    parser.add_argument(
        "-m", "--modality",
        nargs="+",
        choices=["text-only", "image-only", "audio-only", "text-image", "text-audio", "image-audio", "text-image-audio"],
        help="Modality(ies) to run. If not specified, runs all supported modalities for each provider.",
    )
    args = parser.parse_args()

    messages_dir = os.path.join(
        os.path.dirname(__file__),
        "..",
        "tests",
        "pytest",
        "data",
        "generator_messages",
    )
    messages_dir = os.path.abspath(messages_dir)

    output_dir = os.path.join(
        os.path.dirname(__file__),
        "provider_stats",
    )
    output_dir = os.path.abspath(output_dir)

    print(f"Loading messages from: {messages_dir}")
    print(f"Saving stats to: {output_dir}\n")

    # Determine which providers to run
    providers_to_run = args.provider if args.provider else list(PROVIDER_MODALITY_SUPPORT.keys())
    print(f"Providers to run: {providers_to_run}\n")

    # Load existing combined stats if they exist (to append new results)
    combined_output_path = os.path.join(output_dir, "all_provider_stats.json")
    if os.path.exists(combined_output_path):
        with open(combined_output_path, "r") as f:
            all_stats = json.load(f)
        print(f"Loaded {len(all_stats)} existing stats from {combined_output_path}\n")
    else:
        all_stats = {}

    for provider in providers_to_run:
        config = PROVIDER_MODALITY_SUPPORT[provider]
        model = config["model"]
        supported_modalities = config["supported_modalities"]

        # Filter modalities if specified
        if args.modality:
            modalities_to_run = [m for m in args.modality if m in supported_modalities]
            if not modalities_to_run:
                print(f"\nProvider: {provider} - SKIPPED (none of {args.modality} supported)")
                continue
        else:
            modalities_to_run = supported_modalities

        print(f"\nProvider: {provider} (model: {model})")
        print(f"  Modalities to run: {modalities_to_run}")

        for modality in modalities_to_run:
            print(f"\n  Processing modality: {modality}")

            try:
                messages = load_messages(modality, provider, messages_dir)
                print(f"    Loaded {len(messages)} messages from {modality}_{provider}.json")

                stats = capture_stats_for_provider(provider, modality, messages, model)

                output_path = save_stats(stats, output_dir, provider, modality)
                print(f"    Saved to: {output_path}")

                all_stats[f"{provider}_{modality}"] = stats

            except FileNotFoundError as e:
                print(f"    SKIPPED: Message file not found - {e}")
            except Exception as e:
                print(f"    ERROR: {e}")
                import traceback
                traceback.print_exc()

    # Save combined stats (appends to existing)
    with open(combined_output_path, "w") as f:
        json.dump(all_stats, f, indent=2)
    print(f"\nSaved combined stats ({len(all_stats)} entries) to: {combined_output_path}")

    print("\nDone!")


if __name__ == "__main__":
    main()
