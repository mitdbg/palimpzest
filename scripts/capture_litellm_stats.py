#!/usr/bin/env python3
"""
Script to invoke LLM providers through LiteLLM and capture token/cost statistics.

This script:
1. Loads messages from JSON files generated by generate_test_messages.py
2. Sends requests through LiteLLM (the same path palimpzest uses)
3. Saves all usage metadata and response stats returned by LiteLLM
4. Waits 10 seconds
5. Sends the request again and saves the second set of stats

This allows us to compare LiteLLM's reported statistics with:
- Direct provider API calls (from capture_provider_stats.py)
- Palimpzest's generator stats tracking

Supported providers:
- Anthropic: claude-sonnet-4-5-20250929 (text, image, text+image)
- Google/Gemini: gemini-2.5-flash (all seven modality combinations)
- OpenAI: gpt-4o-2024-08-06 (text, image, text+image)
- OpenAI: gpt-4o-audio-preview (text+audio, audio)

Output files are saved to: scripts/litellm_stats/
"""

import argparse
import json
import os
import sys
import time
import uuid
from typing import Any

import litellm
from litellm.integrations.custom_logger import CustomLogger

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from palimpzest.constants import Model


# =============================================================================
# RAW RESPONSE CAPTURE CALLBACK
# =============================================================================
class RawProviderStatsCapture(CustomLogger):
    """
    Custom LiteLLM callback to capture raw provider usage stats before normalization.

    LiteLLM normalizes all responses to OpenAI format, which loses provider-specific
    details like Gemini's per-modality token breakdowns. This callback captures the
    original provider response data.
    """

    def __init__(self):
        self.last_raw_response = None
        self.last_raw_usage = None
        self.last_provider = None

    def log_success_event(self, kwargs, response_obj, start_time, end_time):
        """Called after a successful LLM API call."""
        try:
            # Store the provider info
            self.last_provider = kwargs.get("custom_llm_provider") or kwargs.get("model", "").split("/")[0]

            # Try to get the original response from hidden params
            if hasattr(response_obj, "_hidden_params") and response_obj._hidden_params:
                hidden = response_obj._hidden_params
                self.last_raw_response = hidden.get("original_response")

                # For some providers, the raw response might be in different locations
                if self.last_raw_response is None:
                    self.last_raw_response = hidden.get("raw_response")

            # Try to extract raw usage from the response object itself
            # Some providers have additional attributes that aren't in model_dump()
            if hasattr(response_obj, "_response_ms"):
                if self.last_raw_response is None:
                    self.last_raw_response = {}
                self.last_raw_response["_response_ms"] = response_obj._response_ms

            # For Vertex AI / Gemini, check for provider-specific usage fields
            if hasattr(response_obj, "vertex_ai_usage_metadata"):
                self.last_raw_usage = response_obj.vertex_ai_usage_metadata
            elif hasattr(response_obj, "_vertex_ai_response"):
                self.last_raw_response = response_obj._vertex_ai_response

        except Exception as e:
            # Don't let callback errors break the main flow
            print(f"    [Callback] Error capturing raw response: {e}")

    def log_failure_event(self, kwargs, response_obj, start_time, end_time):
        """Called after a failed LLM API call."""
        self.last_raw_response = None
        self.last_raw_usage = None
        self.last_provider = None

    def reset(self):
        """Reset captured data for next request."""
        self.last_raw_response = None
        self.last_raw_usage = None
        self.last_provider = None

    def get_captured_data(self) -> dict[str, Any]:
        """Return captured raw data and reset for next request."""
        data = {
            "raw_provider_response": self.last_raw_response,
            "raw_provider_usage": self.last_raw_usage,
            "detected_provider": self.last_provider,
        }
        self.reset()
        return data


# Global callback instance
raw_stats_capture = RawProviderStatsCapture()

# Register the callback with LiteLLM
litellm.callbacks = [raw_stats_capture]

# Enable return of response headers (helps with some providers)
litellm.return_response_headers = True


# =============================================================================
# PROVIDER CONFIGURATIONS
# =============================================================================
# Maps provider name to Model enum and supported modalities
# The Model enum is used for:
# 1. Getting the LiteLLM model name via model.value
# 2. Initializing PromptManager which needs a Model enum
PROVIDER_MODALITY_SUPPORT = {
    "anthropic": {
        "model": Model.CLAUDE_4_5_SONNET,
        "supported_modalities": ["text-only", "image-only", "text-image"],
        # Note: Anthropic does not support audio
    },
    "openai": {
        "model": Model.GPT_4o,
        "supported_modalities": ["text-only", "image-only", "text-image"],
    },
    "openai-audio": {
        "model": Model.GPT_4o_AUDIO_PREVIEW,
        "supported_modalities": ["audio-only", "text-audio"],
    },
    "gemini": {
        "model": Model.GOOGLE_GEMINI_2_5_FLASH,
        "supported_modalities": [
            "text-only",
            "image-only",
            "audio-only",
            "text-image",
            "text-audio",
            "image-audio",
            "text-image-audio",
        ],
    },
    "vertex_ai": {
        "model": Model.GEMINI_2_5_FLASH,
        "supported_modalities": [
            "text-only",
            "image-only",
            "audio-only",
            "text-image",
            "text-audio",
            "image-audio",
            "text-image-audio",
        ],
    },
}


def load_messages(modality: str, provider: str, messages_dir: str) -> list[dict]:
    """Load messages from JSON file for a given modality/provider combination."""
    filepath = os.path.join(messages_dir, f"{modality}_{provider}.json")
    with open(filepath, "r") as f:
        return json.load(f)


def transform_messages_for_litellm(messages: list[dict]) -> list[dict]:
    """
    Transform palimpzest message format to LiteLLM-compatible format.

    LiteLLM expects:
    - Messages with role and content
    - Content can be string or list of content blocks
    - No 'type' field at the message level (that's palimpzest-specific)

    This function consolidates multiple user messages with different types
    into single messages with combined content.
    """
    litellm_messages = []

    for msg in messages:
        role = msg.get("role")
        msg_type = msg.get("type")
        content = msg.get("content")

        if role == "system":
            # System messages pass through as-is
            # Content may be string or list of content blocks (for caching)
            litellm_messages.append({"role": "system", "content": content})

        elif role == "user":
            # User messages need consolidation
            if msg_type == "text":
                # Text content - string or list of content blocks
                if litellm_messages and litellm_messages[-1]["role"] == "user":
                    # Merge with existing user message
                    existing = litellm_messages[-1]["content"]
                    if isinstance(existing, str):
                        if isinstance(content, str):
                            litellm_messages[-1]["content"] = [
                                {"type": "text", "text": existing},
                                {"type": "text", "text": content},
                            ]
                        else:
                            litellm_messages[-1]["content"] = [
                                {"type": "text", "text": existing}
                            ] + content
                    else:
                        if isinstance(content, str):
                            existing.append({"type": "text", "text": content})
                        else:
                            existing.extend(content)
                else:
                    litellm_messages.append({"role": "user", "content": content})

            elif msg_type == "image":
                # Image content - list of image_url blocks
                if litellm_messages and litellm_messages[-1]["role"] == "user":
                    existing = litellm_messages[-1]["content"]
                    if isinstance(existing, str):
                        litellm_messages[-1]["content"] = [
                            {"type": "text", "text": existing}
                        ] + content
                    else:
                        existing.extend(content)
                else:
                    litellm_messages.append({"role": "user", "content": content})

            elif msg_type == "input_audio":
                # Audio content - list of input_audio blocks
                if litellm_messages and litellm_messages[-1]["role"] == "user":
                    existing = litellm_messages[-1]["content"]
                    if isinstance(existing, str):
                        litellm_messages[-1]["content"] = [
                            {"type": "text", "text": existing}
                        ] + content
                    else:
                        existing.extend(content)
                else:
                    litellm_messages.append({"role": "user", "content": content})

        elif role == "assistant":
            litellm_messages.append({"role": "assistant", "content": content})

    return litellm_messages


def call_litellm_api(
    messages: list[dict],
    model: Model,
    provider: str,
    cache_key: str | None = None,
) -> dict[str, Any]:
    """
    Call LiteLLM completion API and return all usage statistics.

    This function captures both:
    - Option A: Raw provider usage via callback (if available)
    - Option C: Normalized LiteLLM usage (fallback)

    Args:
        messages: List of message dicts (palimpzest format)
        model: Model enum (used for model name and provider detection)
        provider: Provider name for logging
        cache_key: Optional prompt_cache_key for OpenAI sticky routing to same cache shard

    Returns dict with:
    - usage: Normalized usage dict from LiteLLM response (Option C fallback)
    - usage_raw: Raw provider usage if captured via callback (Option A)
    - response_content: First 200 chars of response
    - model: Model used
    - raw_response: Full response object serialized
    """
    # Reset the callback to capture fresh data for this request
    raw_stats_capture.reset()

    # Transform messages to LiteLLM format
    litellm_messages = transform_messages_for_litellm(messages)

    # Get the LiteLLM model name from the Model enum
    model_name = model.value

    # Set up completion kwargs
    completion_kwargs = {
        "temperature": 0.0,
    }

    # Add modalities for audio models
    if "audio" in model_name.lower():
        completion_kwargs["modalities"] = ["text"]

    # Apply provider-specific caching configuration
    # Messages from generator_messages already have cache_control markers for Anthropic
    if model.is_provider_openai() and cache_key:
        # OpenAI: Use prompt_cache_key for sticky routing to the same cache shard
        # https://platform.openai.com/docs/guides/prompt-caching
        completion_kwargs["extra_body"] = {"prompt_cache_key": cache_key}

    # Make the LiteLLM call
    response = litellm.completion(
        model=model_name,
        messages=litellm_messages,
        **completion_kwargs,
    )

    # ==========================================================================
    # Option C (Fallback): Extract normalized usage stats from LiteLLM response
    # ==========================================================================
    usage_normalized = {}
    if response.usage:
        usage_normalized = response.usage.model_dump()

    # ==========================================================================
    # Option A: Get raw provider data captured by callback
    # ==========================================================================
    callback_data = raw_stats_capture.get_captured_data()
    usage_raw = callback_data.get("raw_provider_usage")

    # Also try to extract raw usage from _hidden_params
    hidden_params = {}
    try:
        if hasattr(response, "_hidden_params") and response._hidden_params:
            hidden_params = dict(response._hidden_params)
            # Some providers store original response here
            if "original_response" in hidden_params:
                original = hidden_params["original_response"]
                if isinstance(original, dict) and "usage_metadata" in original:
                    usage_raw = original["usage_metadata"]
                elif hasattr(original, "usage_metadata"):
                    try:
                        usage_raw = original.usage_metadata.model_dump() if hasattr(original.usage_metadata, "model_dump") else dict(original.usage_metadata)
                    except Exception:
                        pass
    except Exception:
        pass

    # Get response text safely
    try:
        response_text = (
            response.choices[0].message.content[:200]
            if response.choices and response.choices[0].message.content
            else None
        )
    except Exception:
        response_text = None

    # Serialize the full response for debugging
    try:
        raw_response = response.model_dump()
    except Exception:
        raw_response = str(response)

    return {
        "provider": provider,
        "model": model_name,
        "usage": usage_normalized,  # Option C: Normalized LiteLLM format
        "usage_raw": usage_raw,  # Option A: Raw provider format (if captured)
        "response_content": response_text,
        "raw_response": raw_response,
        "hidden_params": hidden_params,
        "callback_data": callback_data,
    }


def capture_stats_for_provider(
    provider: str,
    modality: str,
    messages: list[dict],
    model: Model,
) -> dict[str, Any]:
    """
    Capture stats for a provider by making two requests with a delay.

    Args:
        provider: Provider name (for logging and file naming)
        modality: Modality name
        messages: List of message dicts
        model: Model enum

    Returns dict with:
    - first_request: stats from first request
    - second_request: stats from second request (should show cache hits)
    """
    # Generate a unique cache key for OpenAI (ensures both requests hit the same cache shard)
    # Reference: capture_provider_stats.py and PromptManager.__init__
    openai_cache_key = f"pz-test-{uuid.uuid4().hex[:12]}" if provider in ("openai", "openai-audio") else None

    print(f"    First request...")
    first_stats = call_litellm_api(messages, model, provider, cache_key=openai_cache_key)
    print(f"      Usage: {first_stats['usage']}")

    print(f"    Waiting 20 seconds for cache to be available...")
    time.sleep(20)

    print(f"    Second request (should show cache hits)...")
    second_stats = call_litellm_api(messages, model, provider, cache_key=openai_cache_key)
    print(f"      Usage: {second_stats['usage']}")

    return {
        "provider": provider,
        "model": model.value,
        "modality": modality,
        "first_request": first_stats,
        "second_request": second_stats,
    }


def save_stats(stats: dict[str, Any], output_dir: str, provider: str, modality: str) -> str:
    """Save stats to a JSON file."""
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{provider}_{modality}.json")

    with open(output_path, "w") as f:
        json.dump(stats, f, indent=2, default=str)

    return output_path


def main():
    """Capture LiteLLM stats for supported provider/modality combinations."""
    parser = argparse.ArgumentParser(
        description="Capture token/cost statistics from LLM providers via LiteLLM.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Available providers: {', '.join(PROVIDER_MODALITY_SUPPORT.keys())}
Available modalities: text-only, image-only, audio-only, text-image, text-audio, image-audio, text-image-audio

Examples:
  python capture_litellm_stats.py                              # Run all providers/modalities
  python capture_litellm_stats.py -p openai                    # Run all modalities for OpenAI
  python capture_litellm_stats.py -p openai -m text-only       # Run only text-only for OpenAI
  python capture_litellm_stats.py -p anthropic -m text-image   # Run only text-image for Anthropic
        """
    )
    parser.add_argument(
        "-p", "--provider",
        nargs="+",
        choices=list(PROVIDER_MODALITY_SUPPORT.keys()),
        help="Provider(s) to run. If not specified, runs all providers.",
    )
    parser.add_argument(
        "-m", "--modality",
        nargs="+",
        choices=["text-only", "image-only", "audio-only", "text-image", "text-audio", "image-audio", "text-image-audio"],
        help="Modality(ies) to run. If not specified, runs all supported modalities for each provider.",
    )
    args = parser.parse_args()

    messages_dir = os.path.join(
        os.path.dirname(__file__),
        "..",
        "tests",
        "pytest",
        "data",
        "generator_messages",
    )
    messages_dir = os.path.abspath(messages_dir)

    output_dir = os.path.join(
        os.path.dirname(__file__),
        "litellm_stats",
    )
    output_dir = os.path.abspath(output_dir)

    print(f"Loading messages from: {messages_dir}")
    print(f"Saving stats to: {output_dir}\n")

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Determine which providers to run
    providers_to_run = args.provider if args.provider else list(PROVIDER_MODALITY_SUPPORT.keys())
    print(f"Providers to run: {providers_to_run}\n")

    # Load existing combined stats if they exist (to append new results)
    combined_output_path = os.path.join(output_dir, "all_litellm_stats.json")
    if os.path.exists(combined_output_path):
        with open(combined_output_path, "r") as f:
            all_stats = json.load(f)
        print(f"Loaded {len(all_stats)} existing stats from {combined_output_path}\n")
    else:
        all_stats = {}

    for provider in providers_to_run:
        config = PROVIDER_MODALITY_SUPPORT[provider]
        model = config["model"]
        supported_modalities = config["supported_modalities"]

        # Filter modalities if specified
        if args.modality:
            modalities_to_run = [m for m in args.modality if m in supported_modalities]
            if not modalities_to_run:
                print(f"\nProvider: {provider} - SKIPPED (none of {args.modality} supported)")
                continue
        else:
            modalities_to_run = supported_modalities

        print(f"\nProvider: {provider} (model: {model.value})")
        print(f"  Modalities to run: {modalities_to_run}")

        for modality in modalities_to_run:
            print(f"\n  Processing modality: {modality}")

            try:
                messages = load_messages(modality, provider, messages_dir)
                print(f"    Loaded {len(messages)} messages from {modality}_{provider}.json")

                stats = capture_stats_for_provider(provider, modality, messages, model)

                output_path = save_stats(stats, output_dir, provider, modality)
                print(f"    Saved to: {output_path}")

                all_stats[f"{provider}_{modality}"] = stats

            except FileNotFoundError as e:
                print(f"    SKIPPED: Message file not found - {e}")
            except Exception as e:
                print(f"    ERROR: {e}")
                import traceback
                traceback.print_exc()

    # Save combined stats (appends to existing)
    with open(combined_output_path, "w") as f:
        json.dump(all_stats, f, indent=2, default=str)
    print(f"\nSaved combined stats ({len(all_stats)} entries) to: {combined_output_path}")

    print("\nDone!")


if __name__ == "__main__":
    main()
